{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important note one referencing and copying!\n",
    "When working with a tensor the DEFAULT operations are copy operations,, i.e new memory is allocated for the object\n",
    "we performed the operation on\n",
    "But when we use a method of the tensor class that has an underscore, this references the original memeory address, and hence\n",
    "modifies that value with our operation, this is kind of operation is called an 'in-shap' operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To illustrate the copy aspect\n",
    "\n",
    "t = torch.tensor([\n",
    "  [1,1,1,1],\n",
    "  [2,2,2,2],\n",
    "  [3,3,3,3]\n",
    "],dtype=torch.float32)\n",
    "\n",
    "t.neg()\n",
    "\n",
    "# Our tensor stays positive even after applying the negative element-wise operator\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -1., -1., -1.],\n",
       "        [-2., -2., -2., -2.],\n",
       "        [-3., -3., -3., -3.]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To illustrate the in place operation\n",
    "\n",
    "t = torch.tensor([\n",
    "  [1,1,1,1],\n",
    "  [2,2,2,2],\n",
    "  [3,3,3,3]\n",
    "],dtype=torch.float32)\n",
    "\n",
    "t.neg_()\n",
    "\n",
    "# t stays negative after the operation\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor Ops can be broken down to 3 categories:\n",
    "\n",
    "1. Shaping Operations\n",
    "2. Element-Wise Operations\n",
    "3. Reduction Operations\n",
    "4. Access Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "  [1,1,1,1],\n",
    "  [2,2,2,2],\n",
    "  [3,3,3,3]\n",
    "],dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape determination\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rank Determination\n",
    "len(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Element calculation\n",
    "torch.tensor(t.shape).prod()\n",
    "# or \n",
    "t.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shaping Operations manipulate the axes containing our elements as a means of organizing the tensor,\n",
    "such as when we take images and add axes for color chanels, batch sizes, etc.\n",
    "Reshaping a individual tensor retains the number of elements, but can changes the number of axes and ranks (reshape, squeeze unsqueez, flatten)\n",
    "Combining multiple tensors changes all three aspects (cat, stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without changing the rank. Using -1 lets pytorch calculate the complementary number, if possible\n",
    "t.reshape(-1, 2).shape\n",
    "t.reshape(-1,1).shape\n",
    "t.reshape(-1,12).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Squeezing removes all axes that have a length of 1\n",
    "t.reshape(-1,12).squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 12])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unsqueezing inversely add an axis of length 1\n",
    "t.reshape(-1,12).unsqueeze(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flattening places all elements on one axis\n",
    "t.flatten().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Element-wise operations focus on the specific elements are within the tensor, for these operations, the tensors\n",
    "should have the same shape, and implicitly, the same elements. Examples of these are arithmetic operations (add, sub, mul, etc..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: tensor([[0.1692, 0.9531],\n",
      "        [0.4031, 0.2568]])\n",
      "t2: tensor([[0.8602, 0.8811],\n",
      "        [0.0907, 0.8114]])\n",
      "sum: tensor([[1.0294, 1.8342],\n",
      "        [0.4938, 1.0682]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.rand(2,2)\n",
    "t2 = torch.rand(2,2)\n",
    "print('t1:',t1)\n",
    "print('t2:',t2)\n",
    "t3 = t1+t2\n",
    "print('sum:', t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important concept in element-wise operations is called <b>broadcasting</b>, I'll leave a \"todo\" here to expand on the topic is a huge barrier between the newbies and the pros, and in essense, save a ton on processing and programming, since this\n",
    "would normally be done via a for loop.\n",
    "\n",
    "This knowledge is going to be very handy during data preparation and will be seen more in normalization techniques\n",
    "\n",
    "see: https://deeplizard.com/learn/video/6_33ulFDuCg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.1692, 2.9531],\n",
      "        [2.4031, 2.2568]])\n"
     ]
    }
   ],
   "source": [
    "# Here is the first example of broadcasting, the lower rank tensor, 2, is broadcasted to achieve the shape of \n",
    "# the larger tensor t1\n",
    "t_broad = t1 +2\n",
    "print(t_broad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use comparison operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "  [1,1,1,1],\n",
    "  [2,2,2,2],\n",
    "  [3,3,3,3]\n",
    "],dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.eq(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Less than or equal\n",
    "t.le(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [ True,  True,  True,  True]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# greater than\n",
    "t.gt(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [ True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# greater or equal\n",
    "t.ge(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are a few examples, a important point to note is that these operations are easily computed\n",
    "# using broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also perform basic functions\n",
    "t.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.4142, 1.4142, 1.4142, 1.4142],\n",
       "        [1.7321, 1.7321, 1.7321, 1.7321]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -1., -1., -1.],\n",
       "        [-2., -2., -2., -2.],\n",
       "        [-3., -3., -3., -3.]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.neg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.neg().abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we take a look at reduction operations. As the name implies, reduction operations \n",
    "reduce the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24.)\n",
      "tensor(2.)\n",
      "tensor(1296.)\n",
      "tensor(0.8528)\n"
     ]
    }
   ],
   "source": [
    "# Good examples are the sum, mean, product, and std deviations\n",
    "\n",
    "print(t.sum())\n",
    "print(t.mean())\n",
    "print(t.prod())\n",
    "print(t.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6., 6., 6., 6.])\n",
      "tensor([ 4.,  8., 12.])\n"
     ]
    }
   ],
   "source": [
    "# Where things get intersting (and a little tricky), is when we perform reduction operations\n",
    "# along certain axes.\n",
    "\n",
    "# Here we are summing across the 0 axis = the arrays\n",
    "print(t.sum(dim=0))\n",
    "\n",
    "# Here we are summing across the first axis = the numbers in the arrays\n",
    "print(t.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8)\n",
      "tensor(3.)\n",
      "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "# Another extremely popular operation is called 'argmax', argmax returns the index of the largest value\n",
    "# of the flattened version of the tensor, which is the first three that pops up\n",
    "\n",
    "# We normally will use argmax to determine the classification in the output layer\n",
    "print(t.argmax())\n",
    "\n",
    "# Using the 'max' method shows both the values and the indexes\n",
    "print(t.max())\n",
    "\n",
    "\n",
    "# To drive that point home\n",
    "print(t.flatten())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2207, 0.2967, 0.6653],\n",
       "        [0.0569, 0.3926, 0.3455],\n",
       "        [0.8096, 0.1436, 0.6295]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can perform this operation along a particular axis as well\n",
    "# Lets create a tensor that we can see the results a little more clearly with\n",
    "\n",
    "t = torch.rand(3,3)\n",
    "\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 1, 0])\n",
      "torch.return_types.max(\n",
      "values=tensor([0.8096, 0.3926, 0.6653]),\n",
      "indices=tensor([2, 1, 0]))\n"
     ]
    }
   ],
   "source": [
    "# Using the first dimension, we get the maximum value as it pertains to\n",
    "# a column, where the value is the index of the columns\n",
    "print(t.argmax(dim=0))\n",
    "\n",
    "# Using the 'max' method shows both the values and the indexes\n",
    "print(t.max(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 1, 0])\n",
      "torch.return_types.max(\n",
      "values=tensor([0.6653, 0.3926, 0.8096]),\n",
      "indices=tensor([2, 1, 0]))\n"
     ]
    }
   ],
   "source": [
    "# Using the second dimension, we are given the maximum value of each array along with the index\n",
    "# of that value along that array\n",
    "print(t.argmax(dim=1))\n",
    "print(t.max(dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3956)\n",
      "0.39560940861701965\n",
      "tensor([0.3624, 0.2776, 0.5468])\n",
      "[0.36241066455841064, 0.2776392996311188, 0.54677814245224]\n",
      "0.3956094\n",
      "[0.36241066 0.2776393  0.54677814]\n"
     ]
    }
   ],
   "source": [
    "# the 'item' and 'toList' methods are two methods that can be used turn the tensor into\n",
    "# the actual number type\n",
    "\n",
    "# item pertains to scalar tensors (rank 1)\n",
    "print(t.mean())\n",
    "print(t.mean().item())\n",
    "\n",
    "\n",
    "# to list is applicable to greater rank tensors\n",
    "print(t.mean(dim=0))\n",
    "print(t.mean(dim=0).tolist())\n",
    "\n",
    "# We can also use the numpy method for both of these cases to extract a numpy array\n",
    "print(t.mean().numpy())\n",
    "print(t.mean(dim=0).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b32ba8630eae0cf143c35cc7c8bde428688116bdb8bd122db8a27652bb35d1a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
