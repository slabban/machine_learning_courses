{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important note one referencing and copying!\n",
    "When working with a tensor the DEFAULT operations are copy operations,, i.e new memory is allocated for the object\n",
    "we performed the operation on\n",
    "But when we use a method of the tensor class that has an underscore, this references the original memeory address, and hence\n",
    "modifies that value with our operation, this is kind of operation is called an 'in-shap' operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To illustrate the copy aspect\n",
    "\n",
    "t = torch.tensor([\n",
    "  [1,1,1,1],\n",
    "  [2,2,2,2],\n",
    "  [3,3,3,3]\n",
    "],dtype=torch.float32)\n",
    "\n",
    "t.neg()\n",
    "\n",
    "# Our tensor stays positive even after applying the negative element-wise operator\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -1., -1., -1.],\n",
       "        [-2., -2., -2., -2.],\n",
       "        [-3., -3., -3., -3.]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To illustrate the in place operation\n",
    "\n",
    "t = torch.tensor([\n",
    "  [1,1,1,1],\n",
    "  [2,2,2,2],\n",
    "  [3,3,3,3]\n",
    "],dtype=torch.float32)\n",
    "\n",
    "t.neg_()\n",
    "\n",
    "# t stays negative after the operation\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor Ops can be broken down to 3 categories:\n",
    "\n",
    "1. Shaping Operations\n",
    "2. Element-Wise Operations\n",
    "3. Reduction Operations\n",
    "4. Access Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "  [1,1,1,1],\n",
    "  [2,2,2,2],\n",
    "  [3,3,3,3]\n",
    "],dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape determination\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rank Determination\n",
    "len(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Element calculation\n",
    "torch.tensor(t.shape).prod()\n",
    "# or \n",
    "t.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shaping Operations manipulate the axes containing our elements as a means of organizing the tensor,\n",
    "such as when we take images and add axes for color chanels, batch sizes, etc.\n",
    "Reshaping a individual tensor retains the number of elements, but can changes the number of axes and ranks (reshape, squeeze unsqueez, flatten)\n",
    "Combining multiple tensors changes all three aspects (cat, stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without changing the rank. Using -1 lets pytorch calculate the complementary number, if possible\n",
    "t.reshape(-1, 2).shape\n",
    "t.reshape(-1,1).shape\n",
    "t.reshape(-1,12).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Squeezing removes all axes that have a length of 1\n",
    "t.reshape(-1,12).squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 12])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unsqueezing inversely add an axis of length 1\n",
    "t.reshape(-1,12).unsqueeze(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flattening places all elements on one axis\n",
    "t.flatten().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now talk about 'cat' and 'stack' operations.\n",
    "\n",
    "Concatenate or cat, joins a sequence of tensors along an <b>existing axis</b>\n",
    "\n",
    "Stacking joins a sequence of tensors along a <b>new axis</b>\n",
    "\n",
    "Lets reinforce the importance of this with our go to example of working with image data:\n",
    "\n",
    "1. A single image has three dimensions [channel, height, width], now suppose we had three images and wanted to create a batch with them, this would require a new axis\n",
    "    such that the result is [batch, channel, height , width]. To add that new channel, we need to use <b>stack</b>\n",
    "\n",
    "2. Now that we have our new batch dimension, we would like to concatenate the images together to create a batch size of 3, since we already have our correct dimensions\n",
    "    from the first step, we can use <b>cat</b>\n",
    "\n",
    "Now everytime we wanted to add new images that have [channel, height, width]. We will need to follow steps 1 & 2 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 =torch.tensor([1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall our unsqueeze operation, we will use this shortly\n",
    "print(t1.unsqueeze(dim=0))\n",
    "t1.unsqueeze(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [1],\n",
      "        [1]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall our unsqueeze operation, we will use this shortly\n",
    "print(t1.unsqueeze(dim=1))\n",
    "t1.unsqueeze(dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create three example tensors \n",
    "t1 = torch.tensor([1,1,1])\n",
    "t2 = torch.tensor([2,2,2])\n",
    "t3 = torch.tensor([3,3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 2, 2, 2, 3, 3, 3])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using our single first axis, we can cat the tensors together\n",
    "torch.cat((t1,t2,t3), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [2, 2, 2],\n",
      "        [3, 3, 3]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets now add a new axis at the first index using our stack function \n",
    "# the new axis\n",
    "print(torch.stack((t1,t2,t3), dim=0))\n",
    "torch.stack((t1,t2,t3), dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1],\n",
       "        [2, 2, 2],\n",
       "        [3, 3, 3]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what stack really does is unsqueeze the tensors individually then concatenate that result\n",
    "# this is super useful for getting used to this operation\n",
    "\n",
    "# In this case we turn the tensors in a [1,3] then cat them along the first axis\n",
    "torch.cat(\n",
    "    (t1.unsqueeze(dim=0),\n",
    "    t2.unsqueeze(dim=0),\n",
    "    t3.unsqueeze(dim=0)), dim=0\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [1, 2, 3],\n",
      "        [1, 2, 3]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets now stack our three tensors along the first dimension\n",
    "print(torch.stack((t1,t2,t3), dim=1))\n",
    "torch.stack((t1,t2,t3), dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [1, 2, 3],\n",
       "        [1, 2, 3]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets illustrate this with our cat + unsqueeze combo\n",
    "\n",
    "# In this case we turn the tensors in a [3,1] then cat them along the first axis\n",
    "torch.cat(\n",
    "    (t1.unsqueeze(dim=1),\n",
    "    t2.unsqueeze(dim=1),\n",
    "    t3.unsqueeze(dim=1)), dim=1\n",
    ") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Element-wise operations focus on the specific elements are within the tensor, for these operations, the tensors\n",
    "should have the same shape, and implicitly, the same elements. Examples of these are arithmetic operations (add, sub, mul, etc..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: tensor([[0.1483, 0.7593],\n",
      "        [0.9770, 0.4834]])\n",
      "t2: tensor([[0.5882, 0.2212],\n",
      "        [0.1095, 0.1478]])\n",
      "sum: tensor([[0.7365, 0.9804],\n",
      "        [1.0864, 0.6312]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.rand(2,2)\n",
    "t2 = torch.rand(2,2)\n",
    "print('t1:',t1)\n",
    "print('t2:',t2)\n",
    "t3 = t1+t2\n",
    "print('sum:', t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important concept in element-wise operations is called <b>broadcasting</b>, I'll leave a \"todo\" here to expand on the topic is a huge barrier between the newbies and the pros, and in essense, save a ton on processing and programming, since this\n",
    "would normally be done via a for loop.\n",
    "\n",
    "This knowledge is going to be very handy during data preparation and will be seen more in normalization techniques\n",
    "\n",
    "see: https://deeplizard.com/learn/video/6_33ulFDuCg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.1483, 2.7593],\n",
      "        [2.9770, 2.4834]])\n"
     ]
    }
   ],
   "source": [
    "# Here is the first example of broadcasting, the lower rank tensor, 2, is broadcasted to achieve the shape of \n",
    "# the larger tensor t1\n",
    "t_broad = t1 +2\n",
    "print(t_broad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use comparison operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "  [1,1,1,1],\n",
    "  [2,2,2,2],\n",
    "  [3,3,3,3]\n",
    "],dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.eq(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Less than or equal\n",
    "t.le(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [ True,  True,  True,  True]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# greater than\n",
    "t.gt(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [ True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# greater or equal\n",
    "t.ge(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are a few examples, a important point to note is that these operations are easily computed\n",
    "# using broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also perform basic functions\n",
    "t.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.4142, 1.4142, 1.4142, 1.4142],\n",
       "        [1.7321, 1.7321, 1.7321, 1.7321]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -1., -1., -1.],\n",
       "        [-2., -2., -2., -2.],\n",
       "        [-3., -3., -3., -3.]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.neg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.neg().abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we take a look at reduction operations. As the name implies, reduction operations \n",
    "reduce the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24.)\n",
      "tensor(2.)\n",
      "tensor(1296.)\n",
      "tensor(0.8528)\n"
     ]
    }
   ],
   "source": [
    "# Good examples are the sum, mean, product, and std deviations\n",
    "\n",
    "print(t.sum())\n",
    "print(t.mean())\n",
    "print(t.prod())\n",
    "print(t.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6., 6., 6., 6.])\n",
      "tensor([ 4.,  8., 12.])\n"
     ]
    }
   ],
   "source": [
    "# Where things get intersting (and a little tricky), is when we perform reduction operations\n",
    "# along certain axes.\n",
    "\n",
    "# Here we are summing across the 0 axis = the arrays\n",
    "print(t.sum(dim=0))\n",
    "\n",
    "# Here we are summing across the first axis = the numbers in the arrays\n",
    "print(t.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8)\n",
      "tensor(3.)\n",
      "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "# Another extremely popular operation is called 'argmax', argmax returns the index of the largest value\n",
    "# of the flattened version of the tensor, which is the first three that pops up\n",
    "\n",
    "# We normally will use argmax to determine the classification in the output layer\n",
    "print(t.argmax())\n",
    "\n",
    "# Using the 'max' method shows both the values and the indexes\n",
    "print(t.max())\n",
    "\n",
    "\n",
    "# To drive that point home\n",
    "print(t.flatten())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0885, 0.2731, 0.8982],\n",
       "        [0.0215, 0.0393, 0.0474],\n",
       "        [0.8296, 0.7881, 0.8130]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can perform this operation along a particular axis as well\n",
    "# Lets create a tensor that we can see the results a little more clearly with\n",
    "\n",
    "t = torch.rand(3,3)\n",
    "\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 0])\n",
      "torch.return_types.max(\n",
      "values=tensor([0.8296, 0.7881, 0.8982]),\n",
      "indices=tensor([2, 2, 0]))\n"
     ]
    }
   ],
   "source": [
    "# Using the first dimension, we get the maximum value as it pertains to\n",
    "# a column, where the value is the index of the columns\n",
    "print(t.argmax(dim=0))\n",
    "\n",
    "# Using the 'max' method shows both the values and the indexes\n",
    "print(t.max(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 0])\n",
      "torch.return_types.max(\n",
      "values=tensor([0.8982, 0.0474, 0.8296]),\n",
      "indices=tensor([2, 2, 0]))\n"
     ]
    }
   ],
   "source": [
    "# Using the second dimension, we are given the maximum value of each array along with the index\n",
    "# of that value along that array\n",
    "print(t.argmax(dim=1))\n",
    "print(t.max(dim=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we have Access operations, simply put, access operations pertain to accessing and extracting the value at a tensor\n",
    "The 'item' and 'toList' methods are two methods that can be used turn the tensor into the actual value type ex. ints, floats etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4221)\n",
      "0.4220750033855438\n",
      "tensor([0.3132, 0.3668, 0.5862])\n",
      "[0.3131769299507141, 0.3668476641178131, 0.5862004160881042]\n",
      "0.422075\n",
      "[0.31317693 0.36684766 0.5862004 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# item pertains to scalar tensors (rank 1)\n",
    "print(t.mean())\n",
    "print(t.mean().item())\n",
    "\n",
    "\n",
    "# to list is applicable to greater rank tensors\n",
    "print(t.mean(dim=0))\n",
    "print(t.mean(dim=0).tolist())\n",
    "\n",
    "# We can also use the numpy method for both of these cases to extract a numpy array\n",
    "print(t.mean().numpy())\n",
    "print(t.mean(dim=0).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "87c69a23efa0ee1d5c64982a8e9486c08d519e3aa99d691b2ceccc04345ec1d7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch-fastai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
