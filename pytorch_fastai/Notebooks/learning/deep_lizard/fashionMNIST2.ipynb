{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "\n",
    "from itertools import product\n",
    "from collections import namedtuple\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add regularization, num_workers, CUDA concepts, sequential models, batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data standardization is a specific type of normalization technique. It is sometimes referred to as z-score normalization. The z-score, a.k.a. standard score, is the transformed value for each data point.\n",
    "\n",
    "To normalize a dataset using standardization, we take every value\n",
    "inside the dataset and transform it to its correspondingvalue using the following formula:\n",
    "\n",
    "z = (x - mean)/std\n",
    "\n",
    "After performing this computation on every value inside our dataset, we have a new normalized dataset of values. The mean and standard deviation values are with respect to the dataset as a whole. \n",
    "\n",
    "<i>It's important to note that when we normalize a dataset, we typically group these operations by feature. This means that the mean and standard deviation values are relative to each feature set that's being normalized. If we are working with images, the features are the RGB color channels, so we normalize each color channel with respect to the mean and standard deviation values calculated across all pixels in every images for the respective color channel. In our case we only needs to\n",
    "normalize a single color channel</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slabban/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# Normalization typically occurs at the extraction and transform stages of the ETL process, we can pass the mean and std\n",
    "# via the Normalize method as such:\n",
    "# torchvision.transforms.Normalize(\n",
    "#       [meanOfChannel1, meanOfChannel2, meanOfChannel3] \n",
    "#     , [stdOfChannel1, stdOfChannel2, stdOfChannel3] \n",
    "# )\n",
    "# However, we dont have the mean and std of the channel we are working with and will need to calculate it\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='/home/slabban/machine_learning_courses/datasets'\n",
    "    ,train=True\n",
    "    ,download=True\n",
    "    ,transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ]) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving forward we will start implemention the 'num_workwers' in our dataloaders to increase the speed of our trainings. \n",
    "In a nutshell 'num_workers' specifies the amount of subprocesses can be used to read the data from disk while the main process runs.\n",
    "From the deeplizard course, the biggest improvement came when 1 num workers was added, with diminishing returns as the number as increased.\n",
    "\n",
    "This could be different for other cases, but we will stick to 1 num worker for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2860), tensor(0.3530))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we are dealing with a dataset with a total size that our computer can handle in one run we can simply do this:\n",
    "\n",
    "loader = DataLoader(train_set, batch_size=len(train_set), num_workers=1)\n",
    "images, labels = next(iter(loader))\n",
    "images.mean(), images.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2860), tensor(0.3530))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Often times we will be dealing with huge datasets, we to tackle that case by spliting the set into batches\n",
    "# and implementing the mean and std formulas\n",
    "\n",
    "loader = DataLoader(train_set, batch_size=1000, num_workers=1)\n",
    "\n",
    "num_of_pixels = len(train_set) * 28 * 28\n",
    "total_sum = 0\n",
    "for images, labels in loader: total_sum += images.sum()\n",
    "mean = total_sum / num_of_pixels\n",
    "\n",
    "sum_of_squared_error = 0\n",
    "for images, labels in loader: \n",
    "    sum_of_squared_error += ((images - mean).pow(2)).sum()\n",
    "std = torch.sqrt(sum_of_squared_error / num_of_pixels)\n",
    "\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets now inlcude normalization in our extract and transform steps:\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='/home/slabban/machine_learning_courses/datasets'\n",
    "    ,train=True\n",
    "    ,download=True\n",
    "    ,transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-9.3670e-08), tensor(1.))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The new output of this mean and std is 0 and 1 respectively \n",
    "\n",
    "loader = DataLoader(\n",
    "      train_set\n",
    "    , batch_size=len(train_set)\n",
    "    , num_workers=1\n",
    ")\n",
    "data = next(iter(loader))\n",
    "data[0].mean(), data[0].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch allows us to seamlessly move data to and from our GPU as we preform computations inside our programs.\n",
    "\n",
    "When we go to the GPU, we can use the cuda() method, and when we go to the CPU, we can use the cpu() method.\n",
    "\n",
    "We can also use the to() method. To go to the GPU, we write to('cuda') and to go to the CPU, we write to('cpu'). The to() method is the preferred way mainly because it is more flexible. We'll see one example using using the first two, and then we'll default to always using the to() variant.\n",
    "\n",
    "CPU \tGPU\n",
    "cpu() \tcuda()\n",
    "to('cpu') \tto('cuda')\n",
    "\n",
    "To make use of our GPU during the training process, there are two essential requirements. These requirements are as follows, the data must be moved to the GPU, and the network must be moved to the GPU.\n",
    "\n",
    "    Data on the GPU\n",
    "    Network on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets pull in the very familiar RunManager implementation\n",
    "# We are going to add a simple line at the 'add_graph' at of the tensorboard's 'Summary Writer' instance that will make our Run Manager class\n",
    "# device Agnostic. we are using the getattr() built in function to get the value of the device on the run object. \n",
    "# If the run object doesn't have a device, then cpu is returned. This makes the code backward compatible. \n",
    "# It will still work if we don't specify a device for our run\n",
    "\n",
    "# I will also add some flexibility to the class to allow us to disable tensorboard for file management\n",
    "\n",
    "class RunManager():\n",
    "    def __init__(self, tensorboard=False):\n",
    "        # TODO: extract epoch && run variables into individual classes\n",
    "        self.epoch_count = 0\n",
    "        self.epoch_loss = 0\n",
    "        self.epoch_num_correct = 0\n",
    "        self.epoch_start_time = None\n",
    "\n",
    "        self.run_params = None\n",
    "        self.run_count = 0\n",
    "        self.run_data = []\n",
    "        self.run_start_time = None\n",
    "\n",
    "        self.network = None\n",
    "        self.loader = None\n",
    "\n",
    "        self.istb = tensorboard\n",
    "        self.tb = None\n",
    "\n",
    "        self.tqdm_epoch = None\n",
    "\n",
    "    def begin_run(self, run, network, loader):\n",
    "        self.run_start_time = time.time()\n",
    "        self.run_params = run\n",
    "        self.run_count += 1\n",
    "\n",
    "        self.network = network\n",
    "        self.loader = loader\n",
    "        images, labels = next(iter(self.loader))\n",
    "\n",
    "        grid = torchvision.utils.make_grid(images)\n",
    "        if(self.istb):\n",
    "            self._create_tb(run, grid, images)\n",
    "\n",
    "    def _create_tb(self, run, grid, images):\n",
    "        self.tb = SummaryWriter(comment=f'-{run}')\n",
    "        self.tb.add_image('images', grid)\n",
    "        self.tb.add_graph(self.network, images.to(getattr(run, 'device', 'cpu')))\n",
    "\n",
    "    def end_run(self):\n",
    "        if(self.istb):\n",
    "            self._close_tb()\n",
    "        self.epoch_count = 0\n",
    "    \n",
    "    def _close_tb(self):\n",
    "        self.tb.close()\n",
    "    \n",
    "    def begin_epoch(self):\n",
    "        self.epoch_start_time = time.time()\n",
    "        self.epoch_count += 1\n",
    "        self.epoch_loss = 0\n",
    "        self.epoch_num_correct = 0\n",
    "        self.tqdm_epoch = tqdm(self.loader, unit=\"batch\")\n",
    "        \n",
    "    \n",
    "    def end_epoch(self):\n",
    "        epoch_duration = time.time() - self.epoch_start_time\n",
    "        run_duration = time.time() - self.run_start_time\n",
    "\n",
    "        loss = self.epoch_loss /len(self.loader.dataset)\n",
    "        accuracy = self.epoch_num_correct / len(self.loader.dataset)\n",
    "\n",
    "        if(self.istb):\n",
    "            self._plot_tb(self, loss, accuracy)\n",
    "            \n",
    "\n",
    "        results = OrderedDict()\n",
    "        results[\"run\"] = self.run_count\n",
    "        results[\"epoch\"] = self.epoch_count\n",
    "        results['loss'] = loss\n",
    "        results[\"accuracy\"] = accuracy\n",
    "        results['epoch duration'] = epoch_duration\n",
    "        results['run duration'] = run_duration\n",
    "        for key,val in self.run_params._asdict().items(): results[key] = val\n",
    "        self.run_data.append(results)\n",
    "\n",
    "        df = pd.DataFrame.from_dict(self.run_data, orient='columns')\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        display(df)\n",
    "\n",
    "    def _plot_tb(self, loss, accuracy):\n",
    "        self.tb.add_scalar('Loss', loss, self.epoch_count)\n",
    "        self.tb.add_scalar('Accuracy', accuracy, self.epoch_count)\n",
    "\n",
    "        for name, param in self.network.named_parameters():\n",
    "            self.tb.add_histogram(name, param, self.epoch_count)\n",
    "            self.tb.add_histogram(f'{name}.grad', param.grad, self.epoch_count)\n",
    "        \n",
    "    def track_loss(self, loss, batch):\n",
    "        self.epoch_loss += loss.item() * batch[0].shape[0]\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def track_num_correct(self, preds, labels):\n",
    "        self.epoch_num_correct += self._get_num_correct(preds, labels)\n",
    "    \n",
    "\n",
    "    def _get_num_correct(self, preds, labels):\n",
    "        return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "    \n",
    "    def save(self, fileName):\n",
    "        pd.DataFrame.from_dict(\n",
    "            self.run_data, orient='columns'\n",
    "        ).to_csv(f'{fileName}.csv')\n",
    "\n",
    "        with open(f'{fileName}.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.run_data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets now also pull in our run builder class\n",
    "class RunBuilder():\n",
    "    @staticmethod\n",
    "    def get_runs(params):\n",
    "        # \n",
    "        Run = namedtuple('Run', params.keys())\n",
    "\n",
    "        runs = []\n",
    "        for vals in product(*params.values()):\n",
    "            runs.append(Run(*vals))\n",
    "        return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets also pull in our previous Network\n",
    "\n",
    "# Lets build on our Network class by implementing the 'forward' method, which accepts and returns a tensor\n",
    "# We dont actually call this method ourselves as it is called via the __call__ function in our instantiated layers\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "        \n",
    "    def forward(self, t):\n",
    "       t = self.conv1(t)\n",
    "       t = F.relu(t)\n",
    "       t = F.max_pool2d(t, kernel_size =2, stride=2)\n",
    "\n",
    "       t = self.conv2(t)\n",
    "       t = F.relu(t)\n",
    "       t = F.max_pool2d(t, kernel_size =2, stride=2)\n",
    "\n",
    "       t = t.reshape(-1, 12*4*4)\n",
    "       t = self.fc1(t)\n",
    "       t = F.relu(t)\n",
    "\n",
    "       t = self.fc2(t)\n",
    "       t = F.relu(t)\n",
    "\n",
    "       t = self.out(t)\n",
    "\n",
    "       return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>epoch duration</th>\n",
       "      <th>run duration</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>num_workers</th>\n",
       "      <th>device</th>\n",
       "      <th>shuffle</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.835654</td>\n",
       "      <td>0.678667</td>\n",
       "      <td>12.530017</td>\n",
       "      <td>12.924160</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.455705</td>\n",
       "      <td>0.830933</td>\n",
       "      <td>9.345599</td>\n",
       "      <td>22.495451</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.385629</td>\n",
       "      <td>0.857833</td>\n",
       "      <td>13.978889</td>\n",
       "      <td>36.487157</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.847286</td>\n",
       "      <td>0.666550</td>\n",
       "      <td>10.516921</td>\n",
       "      <td>11.008514</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.467346</td>\n",
       "      <td>0.822267</td>\n",
       "      <td>15.571392</td>\n",
       "      <td>26.594647</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.390981</td>\n",
       "      <td>0.854667</td>\n",
       "      <td>12.499640</td>\n",
       "      <td>39.111908</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.851065</td>\n",
       "      <td>0.678933</td>\n",
       "      <td>24.212804</td>\n",
       "      <td>24.416705</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>cpu</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.452315</td>\n",
       "      <td>0.830883</td>\n",
       "      <td>12.758982</td>\n",
       "      <td>37.190739</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>cpu</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.369348</td>\n",
       "      <td>0.863483</td>\n",
       "      <td>13.746358</td>\n",
       "      <td>50.954662</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>cpu</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.834101</td>\n",
       "      <td>0.686583</td>\n",
       "      <td>12.679285</td>\n",
       "      <td>12.837493</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>cpu</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.421159</td>\n",
       "      <td>0.843917</td>\n",
       "      <td>13.350367</td>\n",
       "      <td>26.204275</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>cpu</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.348954</td>\n",
       "      <td>0.870617</td>\n",
       "      <td>13.252250</td>\n",
       "      <td>39.473531</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>cpu</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    run  epoch      loss  accuracy  epoch duration  run duration    lr  \\\n",
       "0     1      1  0.835654  0.678667       12.530017     12.924160  0.01   \n",
       "1     1      2  0.455705  0.830933        9.345599     22.495451  0.01   \n",
       "2     1      3  0.385629  0.857833       13.978889     36.487157  0.01   \n",
       "3     2      1  0.847286  0.666550       10.516921     11.008514  0.01   \n",
       "4     2      2  0.467346  0.822267       15.571392     26.594647  0.01   \n",
       "5     2      3  0.390981  0.854667       12.499640     39.111908  0.01   \n",
       "6     3      1  0.851065  0.678933       24.212804     24.416705  0.01   \n",
       "7     3      2  0.452315  0.830883       12.758982     37.190739  0.01   \n",
       "8     3      3  0.369348  0.863483       13.746358     50.954662  0.01   \n",
       "9     4      1  0.834101  0.686583       12.679285     12.837493  0.01   \n",
       "10    4      2  0.421159  0.843917       13.350367     26.204275  0.01   \n",
       "11    4      3  0.348954  0.870617       13.252250     39.473531  0.01   \n",
       "\n",
       "    batch_size  num_workers device  shuffle  epochs  \n",
       "0         1000            1   cuda     True       3  \n",
       "1         1000            1   cuda     True       3  \n",
       "2         1000            1   cuda     True       3  \n",
       "3         1000            1   cuda    False       3  \n",
       "4         1000            1   cuda    False       3  \n",
       "5         1000            1   cuda    False       3  \n",
       "6         1000            1    cpu     True       3  \n",
       "7         1000            1    cpu     True       3  \n",
       "8         1000            1    cpu     True       3  \n",
       "9         1000            1    cpu    False       3  \n",
       "10        1000            1    cpu    False       3  \n",
       "11        1000            1    cpu    False       3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here is the updated implementation of the new training loop with our normalized data and GPU implementation\n",
    "# where we pass the network to the device and each of the images and labels to the device on a per-batch basis\n",
    "\n",
    "parameters = OrderedDict(\n",
    "    lr = [.01]\n",
    "    ,batch_size = [1000]\n",
    "    , num_workers = [1]\n",
    "    , device = ['cuda', 'cpu']\n",
    "    , shuffle = [True, False]\n",
    "    , epochs = [3]\n",
    ")\n",
    "\n",
    "manager = RunManager()\n",
    "for run in RunBuilder.get_runs(parameters):\n",
    "    device = torch.device(run.device)\n",
    "    network = Network().to(device)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=run.batch_size, shuffle=run.shuffle)\n",
    "    optimizer = optim.Adam(network.parameters(), lr=run.lr)\n",
    "\n",
    "    total_epochs = range(run.epochs)\n",
    "\n",
    "    manager.begin_run(run, network, train_loader)\n",
    "    for epoch in total_epochs:\n",
    "        manager.begin_epoch()\n",
    "        \n",
    "        for batch in manager.tqdm_epoch:\n",
    "\n",
    "            manager.tqdm_epoch.set_description(f\"Epoch {manager.epoch_count} of {run.epochs}\")\n",
    "            \n",
    "            images = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "\n",
    "            preds = network(images)\n",
    "            loss = F.cross_entropy(preds, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() # calculate gradients\n",
    "            optimizer.step() # update weights\n",
    "\n",
    "            manager.track_loss(loss, batch)\n",
    "            manager.track_num_correct(preds, labels)\n",
    "        manager.end_epoch()\n",
    "    manager.end_run()\n",
    "\n",
    "# Commenting out to prevent file overcrowding\n",
    "#manager.save('results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b32ba8630eae0cf143c35cc7c8bde428688116bdb8bd122db8a27652bb35d1a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
