{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "\n",
    "from itertools import product\n",
    "from collections import namedtuple\n",
    "from collections import OrderedDict"
   ]
  },
  {

   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add regularization, num_workers, and CUDA concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch allows us to seamlessly move data to and from our GPU as we preform computations inside our programs.\n",
    "\n",
    "When we go to the GPU, we can use the cuda() method, and when we go to the CPU, we can use the cpu() method.\n",
    "\n",
    "We can also use the to() method. To go to the GPU, we write to('cuda') and to go to the CPU, we write to('cpu'). The to() method is the preferred way mainly because it is more flexible. We'll see one example using using the first two, and then we'll default to always using the to() variant.\n",
    "\n",
    "CPU \tGPU\n",
    "cpu() \tcuda()\n",
    "to('cpu') \tto('cuda')\n",
    "\n",
    "To make use of our GPU during the training process, there are two essential requirements. These requirements are as follows, the data must be moved to the GPU, and the network must be moved to the GPU.\n",
    "\n",

    "    Data on the GPU\n",
    "    Network on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data standardization is a specific type of normalization technique. It is sometimes referred to as z-score normalization. The z-score, a.k.a. standard score, is the transformed value for each data point.\n",
    "\n",
    "To normalize a dataset using standardization, we take every value\n",
    "inside the dataset and transform it to its correspondingvalue using the following formula:\n",
    "\n",
    "z = (x - mean)/std\n",
    "\n",
    "After performing this computation on every value inside our dataset, we have a new normalized dataset of values. The mean and standard deviation values are with respect to the dataset as a whole. \n",
    "\n",
    "<i>It's important to note that when we normalize a dataset, we typically group these operations by feature. This means that the mean and standard deviation values are relative to each feature set that's being normalized. If we are working with images, the features are the RGB color channels, so we normalize each color channel with respect to the mean and standard deviation values calculated across all pixels in every images for the respective color channel. In our case we only needs to\n",
    "normalize a single color channel</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization typically occurs at the extraction and transform stages of the ETL process, we can pass the mean and std\n",
    "# via the Normalize method as such:\n",
    "# torchvision.transforms.Normalize(\n",
    "#       [meanOfChannel1, meanOfChannel2, meanOfChannel3] \n",
    "#     , [stdOfChannel1, stdOfChannel2, stdOfChannel3] \n",
    "# )\n",
    "# However, we dont have the mean and std of the channel we are working with and will need to calculate it\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='/home/slabban/machine_learning_courses/datasets'\n",
    "    ,train=True\n",
    "    ,download=True\n",
    "    ,transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ]) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2860), tensor(0.3530))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we are dealing with a dataset with a total size that our computer can handle in one run we can simply do this:\n",
    "\n",
    "loader = DataLoader(train_set, batch_size=len(train_set), num_workers=1)\n",
    "images, labels = next(iter(loader))\n",
    "images.mean(), images.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2860), tensor(0.3530))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Often times we will be dealing with huge datasets, we to tackle that case by spliting the set into batches\n",
    "# and implementing the mean and std formulas\n",
    "\n",
    "loader = DataLoader(train_set, batch_size=1000, num_workers=1)\n",
    "\n",
    "num_of_pixels = len(train_set) * 28 * 28\n",
    "total_sum = 0\n",
    "for images, labels in loader: total_sum += images.sum()\n",
    "mean = total_sum / num_of_pixels\n",
    "\n",
    "sum_of_squared_error = 0\n",
    "for images, labels in loader: \n",
    "    sum_of_squared_error += ((images - mean).pow(2)).sum()\n",
    "std = torch.sqrt(sum_of_squared_error / num_of_pixels)\n",
    "\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets now inlcude normalization in our extract and transform steps:\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='/home/slabban/machine_learning_courses/datasets'\n",
    "    ,train=True\n",
    "    ,download=True\n",
    "    ,transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-9.3774e-08), tensor(1.))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Th enew output of this mean and std is 0 and 1 respectively \n",
    "\n",
    "loader = DataLoader(\n",
    "      train_set\n",
    "    , batch_size=len(train_set)\n",
    "    , num_workers=1\n",
    ")\n",
    "data = next(iter(loader))\n",
    "data[0].mean(), data[0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "87c69a23efa0ee1d5c64982a8e9486c08d519e3aa99d691b2ceccc04345ec1d7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch-fastai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
