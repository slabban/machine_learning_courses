{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "758481f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "!pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a0c1866",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastbook import *\n",
    "from IPython.display import display,HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f55cb0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-supervised learning: Training a model using labels that are embedded in the independent \n",
    "# variable, rather than requiring external labels. For instance, training a model to predict the\n",
    "# next word in a text.\n",
    "\n",
    "# Natural Language Processing (NLP) is our first introduction to the world of Self-Supervised Learning\n",
    "# In our exercise, we will be implementing the Universal Language Model Fine-tuning (ULMFit) approach.\n",
    "# The approach uses the pretrained data model for wikipedia text, and trains that model over IMDB\n",
    "# jargon which consists of special words such actors, movies, and directors.\n",
    "# We then train this model our sentiment classifier to determine if the review was positive.\n",
    "\n",
    "# This approach of specializing our model will yield more acurate results compared to training the sentiment\n",
    "# classifier over the wikipedia model.\n",
    "\n",
    "# Since we will be using a lot of the concepts from the previous chapters, lets quickly refersh on \n",
    "# embedding:\n",
    "\n",
    "#     1. Make a list of all possible levels of that categorical variable (we'll call this list the vocab).\n",
    "#     2. Replace each level with its index in the vocab.\n",
    "#     3. Create an embedding matrix for this containing a row for each level (i.e., for each item of the vocab).\n",
    "#     4. Use this embedding matrix as the first layer of a neural network. (A dedicated embedding matrix \n",
    "#        can take as inputs the raw vocab indexes created in step 2; this is equivalent to but faster \n",
    "#        and more efficient than a matrix that takes as input one-hot-encoded vectors representing the indexes.)\n",
    "\n",
    "# We will preserve the embeddings for words that are already exist in the wikipedia model but will \n",
    "# intialize new embeddings for the rows containing vocab from our IMDB set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52cc1fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training on language we will first concatenate the various text files into one large string, and then \n",
    "# seperate the strings into words or characters, called tokens\n",
    "# Our independent variable will be the sequence of words starting with the first word in our very long \n",
    "# list and ending with the second to last, and our dependent variable will be the sequence of words \n",
    "# starting with the second word and ending with the last word.\n",
    "\n",
    "# TODO: Add the jargon terms listed in the book\n",
    "# Tokenization\n",
    "# Numericalization\n",
    "# Language model data loader creation\n",
    "# Language model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a407bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this exercise, we will be using the text library\n",
    "\n",
    "from fastai.text.all import *\n",
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27d5fc88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#7) [Path('/home/slabban/.fastai/data/imdb/tmp_lm'),Path('/home/slabban/.fastai/data/imdb/README'),Path('/home/slabban/.fastai/data/imdb/tmp_clas'),Path('/home/slabban/.fastai/data/imdb/train'),Path('/home/slabban/.fastai/data/imdb/imdb.vocab'),Path('/home/slabban/.fastai/data/imdb/unsup'),Path('/home/slabban/.fastai/data/imdb/test')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f45c4172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/home/slabban/.fastai/data/imdb')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b605d6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'get_text_files' method simplifies the process of grabbing all the text needed, additionally we can add \n",
    "# the folders parameter allows up to choose which folder we would like to grab our text files from\n",
    "\n",
    "files = get_text_files(path, folders = ['train', 'test', 'unsup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc2f4cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Charleton Heston wore one, James Franciscus wore one but Mark Wahlberg opts not to don the'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets take a peek into what we will be tokenizing \n",
    "\n",
    "txt = files[0].open().read(); txt[:90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0eed5c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#219) ['Charleton','Heston','wore','one',',','James','Franciscus','wore','one','but','Mark','Wahlberg','opts','not','to','don','the','traditional','loin','cloth','.','I','hope','no','one','casts','him','as','Tarzan','.'...]\n"
     ]
    }
   ],
   "source": [
    "spacy = WordTokenizer()\n",
    "toks = first(spacy([txt]))\n",
    "print(coll_repr(toks, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c183d3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#9) ['The','U.S.','dollar','$','1','is','$','1.00','.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first(spacy(['The U.S. dollar $1 is $1.00.']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c146a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
