{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Simple module for demonstration\n",
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.param = torch.nn.Parameter(torch.rand(3, 4))\n",
    "        self.linear = torch.nn.Linear(4, 5)\n",
    "        self.linear2 = torch.nn.Linear(5, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x + self.param)\n",
    "        return self.linear2(x).clamp(min=0.0, max=1.0)\n",
    "\n",
    "module = MyModule()\n",
    "\n",
    "from torch.fx import symbolic_trace\n",
    "# Symbolic tracing frontend - captures the semantics of the module\n",
    "symbolic_traced : torch.fx.GraphModule = symbolic_trace(module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %x : [#users=2] = placeholder[target=x]\n",
      "    %param : [#users=1] = get_attr[target=param]\n",
      "    %add : [#users=1] = call_function[target=operator.add](args = (%x, %param), kwargs = {})\n",
      "    %linear : [#users=0] = call_module[target=linear](args = (%add,), kwargs = {})\n",
      "    %linear2 : [#users=1] = call_module[target=linear2](args = (%x,), kwargs = {})\n",
      "    %clamp : [#users=1] = call_method[target=clamp](args = (%linear2,), kwargs = {min: 0.0, max: 1.0})\n",
      "    return clamp\n"
     ]
    }
   ],
   "source": [
    "print(symbolic_traced.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = symbolic_traced.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name     target                   args        kwargs\n",
      "-------------  -------  -----------------------  ----------  ------------------------\n",
      "placeholder    x        x                        ()          {}\n",
      "get_attr       param    param                    ()          {}\n",
      "call_function  add      <built-in function add>  (x, param)  {}\n",
      "call_module    linear   linear                   (add,)      {}\n",
      "call_module    linear2  linear2                  (x,)        {}\n",
      "call_method    clamp    clamp                    (linear2,)  {'min': 0.0, 'max': 1.0}\n",
      "output         output   output                   (clamp,)    {}\n"
     ]
    }
   ],
   "source": [
    "g.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call_module linear (add,)\n",
      "call_module linear2 (x,)\n",
      "call_method clamp (linear2,)\n"
     ]
    }
   ],
   "source": [
    "for node in g.nodes:\n",
    "    if node.op in [\"call_module\", \"call_method\"]:\n",
    "        print(node.op, node.target, node.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def forward(self, x):\n",
      "    param = self.param\n",
      "    add = x + param;  param = None\n",
      "    linear = self.linear(add);  add = None\n",
      "    linear2 = self.linear2(x);  x = None\n",
      "    clamp = linear2.clamp(min = 0.0, max = 1.0);  linear2 = None\n",
      "    return clamp\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(symbolic_traced.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.fx.graph_module.GraphModule.__new__.<locals>.GraphModuleImpl"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(symbolic_traced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.fx.passes.shape_prop import ShapeProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.4654e-01,  1.7194e-02,  6.8955e-02,  2.2021e-01,  4.4075e-02,\n",
       "          8.3911e-02,  7.3841e-02, -5.7304e-02, -2.3597e-01,  2.6656e-01],\n",
       "        [-3.1227e-01, -1.0711e-01,  2.1948e-01,  1.0238e-01, -5.7619e-02,\n",
       "         -1.2323e-01, -2.2494e-01, -4.0803e-01, -6.5411e-02,  4.4413e-01],\n",
       "        [-2.6080e-01,  3.2992e-01,  7.2437e-02, -2.4618e-01, -1.4985e-01,\n",
       "         -2.4337e-01, -3.2790e-02, -4.7158e-01, -4.2521e-01,  1.9706e-01],\n",
       "        [-4.4225e-01, -2.7483e-02, -3.0650e-01,  1.6262e-01, -4.4571e-02,\n",
       "         -1.6837e-01,  1.8660e-01,  3.1474e-02, -4.1161e-01,  4.4892e-01],\n",
       "        [-2.0672e-01, -1.3015e-02, -2.0443e-02,  2.3464e-01,  1.6306e-01,\n",
       "         -8.4241e-02,  1.2211e-01, -1.4450e-01, -1.1165e-01,  7.1051e-01],\n",
       "        [-1.0010e+00, -1.3849e-01,  2.3965e-01,  1.4083e-02,  4.2961e-01,\n",
       "         -1.9452e-01,  2.4543e-01,  1.1455e-01, -2.1688e-01,  3.2175e-01],\n",
       "        [-6.2559e-01,  3.8118e-01, -2.0298e-02,  1.8196e-01,  9.5967e-02,\n",
       "         -1.0640e-01,  8.4253e-01, -1.4863e-01, -3.1846e-01,  6.4177e-01],\n",
       "        [-4.4980e-01,  1.4136e-01,  6.5004e-02, -1.9813e-01, -6.2265e-02,\n",
       "          4.2950e-01,  1.1512e-01, -9.0090e-02, -9.3932e-02,  4.3563e-01],\n",
       "        [-3.4142e-01,  3.6524e-01,  4.0297e-01,  2.0219e-03, -5.2427e-02,\n",
       "         -1.4020e-01,  3.8371e-02,  8.8170e-02, -4.6679e-01,  5.0046e-01],\n",
       "        [-2.3880e-01, -9.3110e-02,  5.1694e-01,  1.0822e-01,  3.0553e-03,\n",
       "         -1.9470e-01,  2.4369e-01, -5.1807e-02, -1.7532e-02,  1.2097e-01],\n",
       "        [-1.5523e-01,  2.8264e-01,  2.3848e-01,  1.5122e-01,  4.2546e-02,\n",
       "         -1.5337e-01,  3.7544e-01, -7.5485e-02, -1.3355e-02,  5.6322e-01],\n",
       "        [-4.1396e-01, -3.2621e-01, -2.1025e-01, -3.1258e-02,  1.4664e-03,\n",
       "         -2.1281e-01, -1.2528e-01, -2.0033e-01, -4.3922e-02, -3.8582e-02],\n",
       "        [-5.3199e-01,  6.7823e-02,  3.5430e-01,  1.8781e-01,  1.7982e-01,\n",
       "          8.8276e-02, -1.0607e-01, -1.6061e-02, -1.4597e-01,  2.3675e-01],\n",
       "        [-3.7226e-01,  4.7564e-02,  2.7679e-01,  1.5131e-01,  5.1368e-02,\n",
       "         -6.1972e-02,  3.4173e-01,  2.0191e-01, -3.4657e-01,  7.1822e-01],\n",
       "        [-3.4386e-01,  1.5087e-01,  3.5797e-01, -1.5844e-01,  1.8084e-01,\n",
       "         -1.3227e-02,  2.7131e-01, -5.1499e-02, -4.5882e-02,  1.5912e-01],\n",
       "        [-3.5020e-01, -1.2615e-02, -1.1792e-01, -1.2728e-01, -1.6177e-01,\n",
       "          2.4279e-01,  2.8298e-01,  1.7624e-01, -3.1893e-01,  3.0770e-01],\n",
       "        [-2.9573e-01,  2.7216e-01,  1.4120e-01,  4.7467e-03, -1.2762e-01,\n",
       "          2.4702e-01, -2.2333e-02, -7.4280e-02, -1.6937e-01,  4.7584e-01],\n",
       "        [-1.9354e-01,  3.2454e-01, -2.5097e-02,  2.8230e-01,  1.1561e-01,\n",
       "          9.1104e-02,  3.3482e-01, -4.5649e-03, -3.0766e-01,  3.8211e-01],\n",
       "        [-2.2392e-01,  1.2476e-01,  3.5727e-01, -8.3526e-02,  5.8097e-02,\n",
       "         -1.5112e-01,  2.6797e-01, -2.1947e-01,  5.5959e-02,  2.2485e-01],\n",
       "        [ 1.9883e-01,  9.7598e-02,  5.2662e-01, -2.5642e-01,  2.5193e-01,\n",
       "         -1.3437e-01,  1.6730e-01, -2.3344e-01,  2.2129e-02,  5.3074e-01],\n",
       "        [-4.9188e-01,  4.4439e-02,  2.8729e-01, -4.9541e-01,  1.2118e-01,\n",
       "         -2.0608e-01,  5.6048e-02, -2.2054e-01, -5.7785e-01,  8.1474e-03],\n",
       "        [-1.7340e-01,  1.6009e-01, -7.1897e-02,  2.1370e-01,  3.1841e-01,\n",
       "          2.5106e-02,  1.8732e-01,  9.0528e-02,  2.0936e-01,  2.5935e-01],\n",
       "        [ 4.8621e-02, -3.6874e-01,  3.4168e-01,  9.4365e-02,  3.3424e-01,\n",
       "         -2.6285e-01,  6.1404e-01, -1.3155e-01,  2.0373e-01,  1.4460e-01],\n",
       "        [-4.8312e-01, -5.1809e-04,  2.0564e-01, -3.8563e-02, -1.9513e-01,\n",
       "         -1.9634e-03,  5.4843e-01, -1.6396e-01, -1.9059e-01,  2.6292e-01],\n",
       "        [-5.0011e-01,  3.8204e-01, -5.8796e-02,  2.9019e-01,  2.3170e-01,\n",
       "         -4.3999e-03,  2.0916e-02, -6.0404e-02, -1.5919e-01,  6.7206e-01],\n",
       "        [-2.6806e-01, -3.4297e-01,  8.7974e-02, -2.3710e-02, -5.7109e-02,\n",
       "         -8.8398e-02,  8.4019e-02, -4.6034e-02, -1.8359e-01,  2.2788e-01],\n",
       "        [-4.0022e-01,  6.3480e-02,  3.4743e-01,  1.1363e-01,  8.9043e-02,\n",
       "         -1.9316e-02,  2.1177e-01,  1.3150e-01, -3.5263e-01,  5.0971e-01],\n",
       "        [-2.6912e-02, -1.5166e-01,  3.8085e-01, -2.4408e-01,  8.6590e-02,\n",
       "         -8.1660e-02,  2.5499e-02, -1.0530e-01, -2.0133e-01,  9.2139e-02],\n",
       "        [-4.5961e-01,  1.7979e-01,  4.2945e-01, -6.2515e-02, -6.4725e-02,\n",
       "          1.0577e-01, -7.0103e-02, -7.5448e-02, -1.6889e-02,  1.8257e-01],\n",
       "        [-1.8962e-01, -3.6168e-02,  2.0134e-01,  2.8149e-01, -5.0971e-02,\n",
       "          1.8821e-01,  2.3942e-01, -1.3255e-01, -2.7817e-01,  5.1338e-01],\n",
       "        [-2.9281e-01,  2.4230e-01,  3.6836e-01, -1.3856e-01, -1.9868e-01,\n",
       "          1.5606e-01,  1.7596e-01, -1.1774e-01, -2.1028e-01,  4.6042e-01],\n",
       "        [-6.8573e-01, -1.5212e-01,  1.3189e-01, -1.0261e-01,  3.4377e-01,\n",
       "         -1.9288e-03,  4.3863e-01, -2.7472e-01,  7.2997e-02,  2.1515e-01],\n",
       "        [-1.1765e-01,  2.5506e-02,  1.8412e-01,  1.2697e-01, -1.8025e-02,\n",
       "         -6.4102e-02,  1.4864e-02, -8.7659e-02,  5.6222e-02,  1.1989e-01],\n",
       "        [-1.5853e-01,  4.7203e-01,  1.7735e-01,  3.5881e-03,  1.6817e-01,\n",
       "          9.9098e-02,  2.4535e-01,  8.1512e-02, -3.8975e-01,  1.4071e-01],\n",
       "        [-4.2513e-01,  5.3865e-01,  2.1309e-02, -1.1346e-01,  9.4929e-02,\n",
       "          2.1678e-01,  3.2911e-01, -2.3373e-01, -4.6194e-01,  6.2255e-01],\n",
       "        [-5.4362e-01,  5.6615e-01,  4.8172e-01,  9.0190e-03, -1.5374e-01,\n",
       "          3.0318e-02,  1.6983e-01, -1.1828e-01, -5.1260e-01,  4.6988e-01],\n",
       "        [-5.2888e-01, -5.5304e-02,  3.5371e-01,  1.5078e-02, -3.1295e-01,\n",
       "          1.7355e-01,  6.7871e-01, -2.1473e-02, -1.6323e-01,  2.0291e-01],\n",
       "        [-5.2487e-01,  6.4701e-02,  2.6106e-01, -7.3763e-02,  2.0465e-01,\n",
       "         -1.4763e-01,  3.6267e-01, -4.4494e-01, -7.7624e-02,  4.3241e-01],\n",
       "        [-4.8821e-01, -1.2776e-01,  2.4662e-01, -3.3602e-02,  7.3505e-02,\n",
       "         -2.9759e-01,  7.0730e-01,  1.2348e-01,  5.4016e-02,  3.6705e-01],\n",
       "        [-4.6298e-01,  3.9914e-01,  1.6683e-01, -1.6208e-02,  2.9454e-01,\n",
       "         -1.0320e-01,  2.5131e-01, -2.7706e-01, -5.1632e-01,  5.5053e-01],\n",
       "        [-1.6956e-01, -3.3744e-02,  3.2395e-01,  1.2169e-01, -1.3406e-01,\n",
       "         -3.2391e-01,  8.9892e-02,  3.1818e-01, -3.9439e-01,  4.5329e-01],\n",
       "        [-1.5249e-01,  5.9478e-02,  2.0918e-01,  1.8420e-01,  1.3322e-01,\n",
       "          9.7243e-02,  1.1569e-01,  1.2631e-01,  3.4077e-01,  4.1123e-01],\n",
       "        [-3.4123e-01,  3.8349e-01,  1.0596e-01, -1.7944e-01,  8.9281e-02,\n",
       "          8.4423e-02,  2.6035e-01, -5.6719e-02, -3.1552e-01,  4.4221e-01],\n",
       "        [-3.3599e-01, -1.3920e-01, -7.8467e-02,  8.6531e-02, -5.0592e-03,\n",
       "         -1.5338e-01,  1.2142e-01,  2.0958e-02, -3.0931e-01,  2.8398e-01],\n",
       "        [-3.1936e-01,  5.0119e-02,  1.3363e-01,  2.2141e-01, -1.2272e-01,\n",
       "          1.1099e-01,  3.0953e-02, -6.6755e-03, -4.1538e-01,  6.3589e-01],\n",
       "        [ 8.7732e-02, -1.1016e-02,  3.5946e-01,  1.5287e-01,  2.2911e-01,\n",
       "         -2.4903e-01, -8.0746e-02, -2.1246e-01, -1.3085e-01,  4.5902e-01],\n",
       "        [-6.5427e-03,  1.0785e-01,  8.0613e-02, -1.2385e-01, -2.0016e-01,\n",
       "         -3.9061e-02,  1.7664e-01, -1.4930e-01, -5.2166e-01, -1.3927e-01],\n",
       "        [-2.0273e-01,  2.4209e-01,  1.7170e-01, -1.4300e-01, -1.3389e-01,\n",
       "          4.2652e-02,  2.4340e-01, -3.3257e-01, -5.4537e-01,  1.9980e-01],\n",
       "        [ 3.7477e-02,  1.5128e-01,  2.4655e-01, -9.2802e-02,  4.3631e-01,\n",
       "         -3.7588e-02,  2.6073e-01,  1.5114e-01, -1.9720e-01,  5.0699e-01],\n",
       "        [-1.6425e-01,  4.7854e-02,  4.3379e-01, -2.1866e-01,  1.1422e-01,\n",
       "         -2.6640e-01,  3.8241e-01, -1.6138e-02,  8.3322e-02,  1.1236e-01]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "    def forward(self, x):\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "gm = torch.fx.symbolic_trace(model)\n",
    "sample_input = torch.randn(50, D_in)\n",
    "ShapeProp(gm).propagate(sample_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.float32 torch.Size([50, 1000])\n",
      "linear1 torch.float32 torch.Size([50, 100])\n",
      "clamp torch.float32 torch.Size([50, 100])\n",
      "linear2 torch.float32 torch.Size([50, 10])\n",
      "output torch.float32 torch.Size([50, 10])\n"
     ]
    }
   ],
   "source": [
    "for node in gm.graph.nodes:\n",
    "    print(node.name, node.meta['tensor_meta'].dtype,\n",
    "        node.meta['tensor_meta'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ligeng/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-3.3239e-02, -7.5997e-01, -1.3424e-01, -1.2014e-01, -4.2429e-02,\n",
       "          5.7649e-02,  5.2269e-01, -7.8221e-01, -3.5845e-01, -2.2237e-01,\n",
       "         -1.0024e-01,  5.4364e-01, -1.8134e-01, -8.0633e-01,  4.2117e-01,\n",
       "          3.8587e-01, -3.7670e-01,  7.7968e-01,  3.3259e-01,  1.6050e-01,\n",
       "          1.1636e-01, -2.6960e-01,  2.0935e-01,  1.1797e-02, -3.2981e-01,\n",
       "         -8.8132e-02,  8.8123e-02,  6.5380e-01, -2.5806e-01,  1.6193e-01,\n",
       "         -3.9047e-01, -4.8978e-01, -1.4013e-01, -1.8331e-01, -4.8718e-02,\n",
       "          6.3806e-02,  6.9854e-01, -9.1238e-01,  2.4740e-01, -6.3716e-01,\n",
       "          6.2757e-01, -7.6974e-01, -8.5881e-01, -7.0892e-02,  3.4040e-01,\n",
       "          3.7809e-01, -6.4334e-01, -4.5084e-01, -3.6080e-01,  4.0448e-01,\n",
       "         -7.0230e-02,  2.6103e-01, -3.8110e-02,  3.8979e-01,  3.4592e-01,\n",
       "         -3.2595e-01,  3.2237e-01,  5.0795e-01,  6.2589e-01,  4.0218e-01,\n",
       "         -3.5472e-01, -3.0060e-01, -6.5672e-01, -3.1811e-01,  2.5244e-01,\n",
       "          8.0378e-02, -8.3943e-01,  1.7098e-01,  5.5227e-01,  4.5124e-01,\n",
       "         -3.1035e-01,  5.4132e-01, -1.1063e-01,  2.6528e-01,  6.0761e-01,\n",
       "         -2.7300e-01, -5.1382e-02,  5.9803e-01, -4.8788e-01,  5.7511e-02,\n",
       "         -4.1639e-01,  4.0677e-01, -3.8668e-02, -3.9426e-01, -4.4770e-01,\n",
       "          1.1844e+00, -6.2087e-01, -5.8912e-01, -1.0445e-01, -7.1112e-01,\n",
       "          6.0081e-02, -4.8309e-01, -3.2971e-01,  6.1992e-01,  1.9016e-01,\n",
       "         -2.0989e-01,  1.2893e-01,  8.1087e-02, -1.9910e-01, -6.8664e-01,\n",
       "          3.9428e-01, -2.5170e-01,  9.2776e-02,  8.0365e-02, -4.2456e-01,\n",
       "          9.9231e-01, -1.8373e-01, -2.0796e-01, -5.4302e-01, -3.7664e-01,\n",
       "          6.3692e-01, -2.4779e-01,  2.8664e-01,  3.9672e-01, -2.5062e-01,\n",
       "          4.8500e-01, -2.8948e-01,  1.4120e-01, -5.4104e-01, -3.2901e-01,\n",
       "         -1.8557e-01,  1.5677e-01,  6.0298e-01, -4.9315e-01, -1.1815e-01,\n",
       "          1.1734e-01, -8.8970e-01,  4.2566e-01, -3.6435e-01, -1.1352e-01,\n",
       "          2.8726e-01, -8.1614e-01,  4.1014e-01, -3.3435e-01,  4.8416e-01,\n",
       "         -1.3720e-01,  4.2034e-01,  6.8453e-01,  1.6956e-01,  1.3813e-01,\n",
       "          3.0910e-02, -1.2260e+00,  2.8716e-01, -9.9024e-01,  6.2744e-01,\n",
       "         -7.4191e-01, -1.5230e-01,  6.5194e-01,  2.2407e-01,  9.0620e-01,\n",
       "         -4.2000e-02,  3.5354e-01, -4.9801e-01,  2.3306e-01, -4.6323e-01,\n",
       "         -1.5450e-01, -6.0503e-01,  4.7223e-01, -7.1176e-01,  1.4322e-01,\n",
       "          1.8990e-01,  4.2187e-01,  2.1940e-01, -1.0638e+00,  1.4646e-01,\n",
       "         -8.7621e-01, -8.3083e-02,  3.9503e-01, -9.7228e-02,  7.7389e-02,\n",
       "          2.4967e-01,  9.5565e-02, -5.2592e-01, -3.1514e-01,  4.2292e-02,\n",
       "         -1.1004e-01, -5.7944e-01, -1.9559e-01,  4.1405e-04,  3.2830e-01,\n",
       "         -7.7220e-01, -1.0206e-01, -4.4499e-02, -2.0686e-01, -3.5459e-01,\n",
       "          3.0662e-02, -4.4947e-01,  1.2086e-01,  1.4169e-01, -2.4061e-01,\n",
       "          5.6126e-02, -4.8236e-01, -8.3857e-01,  2.5245e-01,  7.4172e-02,\n",
       "         -3.0408e-01, -1.6097e-01,  6.6267e-02, -5.6644e-01,  4.1250e-01,\n",
       "          1.3482e+00, -9.6130e-01,  6.5155e-01, -2.4160e-01,  6.0213e-01,\n",
       "          9.0356e-01,  3.3461e-01, -8.8135e-03,  2.4308e-01,  2.6526e-01,\n",
       "          3.5572e-01,  4.6582e-01, -1.6908e-01, -9.1248e-02, -1.6834e-01,\n",
       "          3.8350e-02, -1.5046e-01, -4.8177e-01,  7.8604e-01,  3.7608e-01,\n",
       "          2.6357e-01, -9.3965e-01,  5.9108e-01,  2.2659e-01,  3.4757e-01,\n",
       "         -7.0579e-01,  4.7429e-01, -5.6355e-01,  1.6768e-01,  3.1999e-01,\n",
       "          4.1854e-01, -2.7005e-01, -4.0026e-01,  8.5444e-02, -1.1448e+00,\n",
       "          1.9965e-01, -6.8722e-01,  2.9670e-01, -2.8502e-01,  3.4947e-01,\n",
       "         -9.3573e-01,  1.3482e-01,  2.6954e-01, -1.2150e+00,  9.1441e-02,\n",
       "         -5.6558e-01,  4.2850e-01,  1.8579e-01,  5.1384e-01,  1.0744e+00,\n",
       "         -5.1378e-01,  4.5031e-01,  1.7604e-01,  1.6316e-01,  4.8975e-01,\n",
       "          2.9963e-01, -3.6449e-01, -1.7343e-01, -3.5970e-01, -1.4514e-01,\n",
       "          9.1827e-03,  5.7602e-01, -2.4657e-02, -2.5464e-01, -6.5557e-02,\n",
       "          3.5901e-01, -1.7870e-01,  2.5605e-01, -3.0216e-01, -1.1436e-01,\n",
       "          3.6308e-01,  1.7480e-01, -3.3577e-01,  4.3454e-01,  3.4790e-01,\n",
       "          4.2643e-01,  6.1159e-02, -2.5794e-01, -2.2478e-01,  6.1292e-01,\n",
       "         -2.4425e-01,  3.6176e-02, -4.9301e-01,  4.4602e-01,  1.1539e+00,\n",
       "          8.6235e-01, -4.6926e-01,  3.7221e-01, -5.3782e-01, -1.5229e-01,\n",
       "          4.2239e-01,  2.1065e-01,  4.8164e-01, -7.1005e-01, -1.0733e-01,\n",
       "          5.9487e-01, -2.3793e-01, -5.7781e-01, -3.7645e-01, -5.3963e-01,\n",
       "         -5.4765e-01, -8.9019e-01, -6.3038e-02,  1.7410e-01, -5.0873e-03,\n",
       "         -5.3728e-02, -2.9630e-01,  1.1945e-01, -1.7415e-01,  3.2697e-01,\n",
       "          6.3234e-03, -2.6818e-01,  3.8776e-01,  2.8162e-01,  3.0568e-01,\n",
       "          1.7378e-01, -3.5480e-01, -6.9219e-01, -4.7017e-01,  2.2444e-01,\n",
       "         -4.6167e-01,  4.1396e-02, -7.8155e-01,  6.0057e-01,  4.0273e-01,\n",
       "         -3.0736e-01, -3.5404e-01,  5.5324e-01, -5.4992e-01, -1.3866e-01,\n",
       "          3.8918e-01,  7.2099e-01,  6.4134e-01, -3.5137e-01, -1.4564e-01,\n",
       "         -1.5199e-01, -6.0137e-01, -1.5349e-01, -3.1561e-01, -2.1124e-01,\n",
       "         -6.5095e-01, -1.6882e-01,  2.7536e-01, -3.4131e-01,  1.0838e-02,\n",
       "         -1.8201e-01, -1.1946e-01, -3.6017e-01, -1.2814e+00,  2.0975e-01,\n",
       "          2.5517e-01, -5.3822e-01,  3.4508e-01,  2.7278e-01, -2.3317e-01,\n",
       "         -5.2391e-01,  7.2550e-02, -2.7582e-01, -6.8265e-02, -5.5765e-01,\n",
       "          2.5876e-01,  1.3576e-01,  3.7724e-01,  1.6653e-01,  2.2599e-01,\n",
       "         -5.1931e-01,  1.8091e-01,  1.4610e-02, -3.4762e-01,  3.7076e-01,\n",
       "          3.6247e-02,  5.1317e-01,  3.6261e-01,  4.2054e-01, -2.7973e-01,\n",
       "         -5.6298e-01, -3.7089e-01, -4.6363e-01, -3.2747e-01,  4.6060e-01,\n",
       "         -2.0036e-01,  1.8679e-01,  5.7866e-01,  1.0549e+00,  8.5920e-02,\n",
       "          1.0267e+00,  1.0974e+00,  4.6861e-01,  1.6047e-01, -9.7494e-02,\n",
       "          4.6650e-01, -4.4867e-01, -7.8972e-01,  8.9232e-01, -3.5236e-01,\n",
       "         -1.4810e-01, -3.2138e-01, -4.7365e-01, -6.4004e-01, -3.8719e-01,\n",
       "          1.8419e-03, -4.3641e-02,  3.8718e-01, -3.6785e-01,  4.3125e-01,\n",
       "          4.9339e-01, -4.7895e-01,  1.8484e-01, -9.6946e-02,  5.6847e-01,\n",
       "         -6.8122e-01,  1.5667e-02, -3.5312e-01,  7.6775e-02,  5.0011e-01,\n",
       "          5.2819e-01,  9.2078e-01,  2.1959e-01,  1.2294e-01,  9.6343e-01,\n",
       "          2.1881e-01,  7.1138e-02,  7.1068e-01, -5.1489e-01,  3.0917e-01,\n",
       "          3.2191e-01, -2.3403e-01, -3.0361e-02,  7.4161e-01, -1.2803e+00,\n",
       "          1.6795e-01, -1.1582e+00,  1.3158e-01,  1.4727e-01, -1.1975e-01,\n",
       "          3.5684e-01, -8.7596e-02, -5.4101e-01,  4.8903e-01,  6.8554e-01,\n",
       "         -6.9321e-02, -3.8180e-01, -6.4629e-02,  4.7245e-01, -1.4630e-01,\n",
       "         -5.1978e-01,  2.8213e-01,  2.6600e-01, -2.8072e-01,  4.2042e-03,\n",
       "          2.1317e-01,  1.2059e-01, -4.4771e-01, -3.1345e-02,  5.2826e-01,\n",
       "         -6.8012e-01,  1.2268e+00, -3.1764e-01, -1.6898e-01, -2.9674e-01,\n",
       "          3.0346e-01, -2.0146e-02,  1.2219e-01,  1.6406e-01, -2.1209e-01,\n",
       "         -5.4516e-01, -1.7986e-01, -2.2819e-01, -1.0211e-01, -4.8104e-01,\n",
       "         -1.5372e-01,  2.8384e-02, -1.6795e-01, -3.8858e-01, -1.6308e-01,\n",
       "          1.5058e-01, -4.2897e-02,  5.0579e-01, -3.2487e-01, -2.0631e-01,\n",
       "         -5.5453e-01,  9.1831e-02,  5.0878e-01, -3.9718e-01, -5.7928e-01,\n",
       "          5.9327e-01, -3.4175e-01, -5.0572e-01,  9.1164e-02,  1.1348e+00,\n",
       "          1.0226e-01,  4.0109e-01, -3.2373e-01,  1.7664e-01,  3.4790e-01,\n",
       "         -4.7316e-01, -3.6346e-02, -9.0643e-02, -4.6406e-01, -2.5706e-01,\n",
       "         -5.9378e-01,  4.7349e-02, -5.9237e-01, -1.0487e-01, -9.0159e-01,\n",
       "          4.6295e-01, -1.1107e+00, -2.1534e-01, -4.8224e-01, -3.0602e-01,\n",
       "         -4.5596e-01,  3.4724e-01,  2.7252e-01, -1.2520e-01, -1.1846e-01,\n",
       "          1.0317e-01, -5.3239e-01,  1.6833e-01, -7.2525e-02, -7.4635e-01,\n",
       "         -2.5803e-01,  6.5758e-01,  1.0644e-01,  9.7879e-02,  4.0977e-01,\n",
       "          4.5343e-01, -2.2511e-01,  6.2890e-01,  7.5342e-03, -4.0825e-01,\n",
       "          8.4230e-01, -7.8890e-02, -3.9807e-01,  1.0320e+00, -7.0408e-02,\n",
       "         -6.8311e-01,  5.7102e-01, -4.2176e-01,  2.1330e-01, -4.4340e-01,\n",
       "         -2.2102e-02, -1.4323e-01, -1.1715e-01, -3.0651e-01, -2.7509e-01,\n",
       "         -5.3187e-01,  1.3181e-01,  4.7976e-01, -6.6105e-01, -4.8744e-01,\n",
       "         -3.0760e-02, -1.8212e-01, -7.3769e-02, -1.1769e-01,  1.9725e-01,\n",
       "         -2.0891e-02, -2.8015e-01,  4.2900e-02, -3.0980e-01,  5.4697e-01,\n",
       "         -2.1541e-01,  3.9202e-01, -1.2283e+00,  4.8050e-03, -4.4466e-01,\n",
       "          5.5902e-02,  2.6451e-01,  3.8177e-01,  1.2726e-01, -4.7984e-02,\n",
       "         -1.6559e-01, -3.3166e-01,  1.0658e+00,  3.7146e-01, -2.9503e-01,\n",
       "         -3.0257e-02,  4.1211e-01, -4.4442e-01,  3.4021e-01, -4.7455e-01,\n",
       "         -1.0915e+00,  2.3934e-01, -4.7014e-02, -1.0501e+00, -6.9870e-01,\n",
       "          1.5853e-02, -3.8371e-01,  1.9187e-01,  3.0071e-01, -2.5656e-01,\n",
       "          4.7140e-01, -1.7690e-01,  5.4132e-01, -2.3061e-01,  8.9258e-01,\n",
       "         -7.6767e-01,  1.0819e-01,  8.8273e-01, -3.7468e-01, -2.9480e-01,\n",
       "         -8.7038e-02,  9.3979e-02, -1.3734e-01, -4.1340e-02,  3.3363e-01,\n",
       "          3.3657e-01, -2.5576e-01,  8.0924e-02,  3.0746e-01,  1.1630e-01,\n",
       "          7.5662e-01, -1.5539e-01, -6.3374e-02, -5.2007e-01,  1.1144e+00,\n",
       "          1.0146e+00, -5.4338e-01,  1.6533e-01,  3.0587e-01, -1.0560e+00,\n",
       "         -1.3293e-02,  2.6298e-01,  1.2557e-01, -1.1032e+00, -5.9410e-01,\n",
       "         -1.1075e-01, -4.1198e-01,  1.8802e-01, -1.3644e-01,  1.7101e-01,\n",
       "          5.1889e-01,  2.2512e-01,  4.2216e-01,  5.3466e-01,  4.6298e-01,\n",
       "         -5.5937e-01, -4.2551e-01, -9.8747e-01, -4.1504e-01,  5.6755e-01,\n",
       "         -1.5045e-01, -3.6743e-01,  7.6466e-02,  1.1609e+00,  3.4160e-01,\n",
       "         -4.8318e-01, -1.4120e-01,  7.7239e-01,  6.0816e-01, -3.3598e-01,\n",
       "          3.3142e-02,  2.8992e-01, -4.5408e-01,  1.1555e-01, -2.9273e-01,\n",
       "          2.0393e-01,  5.5791e-01, -2.3771e-01,  4.6823e-01, -1.5303e-01,\n",
       "         -1.2522e-01, -6.2295e-01,  3.8936e-01, -2.6587e-01,  1.9089e-01,\n",
       "          6.3557e-01, -4.6125e-01,  9.3278e-02,  7.3828e-01,  4.0311e-01,\n",
       "          4.9375e-02, -7.5918e-03,  3.9515e-01,  1.9284e-01,  1.6160e-01,\n",
       "          9.5680e-01,  1.6782e-02,  1.0706e-01, -5.7210e-01,  3.0495e-01,\n",
       "          2.9866e-01, -8.7969e-01, -2.8915e-01, -6.1755e-01,  2.2032e-02,\n",
       "         -2.7805e-01,  6.9041e-01, -4.7054e-01, -2.5293e-01, -6.9018e-01,\n",
       "          7.9158e-01, -7.6711e-02, -3.2213e-01,  4.5382e-01,  2.1133e-01,\n",
       "         -5.6832e-01,  2.5114e-01,  1.3876e-01,  2.5132e-02,  5.5747e-01,\n",
       "          1.4577e-01, -7.7032e-01,  2.0110e-01, -7.5838e-01,  1.6192e-01,\n",
       "         -5.8420e-01,  2.9489e-02,  9.7371e-01, -4.9209e-02,  1.8502e-01,\n",
       "          1.9193e-01, -2.4638e-01,  6.4449e-01, -6.2453e-01, -3.8470e-01,\n",
       "         -6.2660e-01,  6.1057e-01,  1.1205e-01, -6.1653e-01,  5.9072e-01,\n",
       "         -2.0475e-03,  3.4199e-01,  4.5679e-01,  6.1778e-01, -6.7740e-03,\n",
       "         -1.3846e-01, -6.5162e-02,  1.7117e-01,  3.5055e-01,  4.6431e-01,\n",
       "         -3.5389e-02,  5.6268e-01, -5.4628e-03,  1.3238e-01,  7.6951e-02,\n",
       "         -1.1122e-01, -6.8423e-02, -6.3939e-01, -5.1408e-01, -9.4511e-02,\n",
       "          2.5473e-02,  7.2174e-02,  4.6990e-01, -1.4603e-01, -1.6468e-01,\n",
       "         -4.0315e-01,  1.7515e-01,  4.7302e-02,  3.9215e-01,  7.2615e-01,\n",
       "          5.3189e-01,  2.8232e-01, -2.2580e-01,  9.6772e-02,  4.9054e-01,\n",
       "         -6.0120e-01, -3.6466e-01,  3.8502e-01, -5.7131e-01,  6.0513e-01,\n",
       "          1.7957e-01, -1.5214e-01, -9.8320e-01,  8.3993e-02, -4.1276e-01,\n",
       "          5.4286e-01,  3.0482e-01,  3.2836e-01, -4.2939e-03, -7.8988e-02,\n",
       "          5.5783e-01,  1.5437e-01,  1.3153e+00, -4.0890e-01,  6.8984e-03,\n",
       "         -4.0749e-01,  4.3057e-01,  8.4587e-01,  9.2302e-02, -1.2930e-01,\n",
       "          4.2323e-01, -9.3066e-01, -2.7820e-01,  5.8407e-02, -9.6898e-02,\n",
       "         -8.4029e-01,  2.3922e-01,  8.8497e-02,  1.6898e-01,  1.5638e-01,\n",
       "         -7.4774e-01,  3.1978e-01, -4.8056e-02, -3.6236e-01,  7.4369e-01,\n",
       "         -4.0904e-01,  4.3050e-01, -2.7613e-02, -6.9685e-01,  6.2689e-01,\n",
       "          4.6583e-01,  4.6360e-01, -7.8368e-01,  6.6237e-01,  2.4068e-01,\n",
       "          6.5565e-01, -6.5207e-01, -3.8353e-01,  2.7876e-02, -4.5107e-01,\n",
       "          1.8789e-01,  4.2303e-01,  6.2778e-01, -3.5542e-02, -3.2523e-01,\n",
       "         -4.9477e-01, -2.9718e-01,  5.9752e-01, -2.4672e-01,  5.9725e-01,\n",
       "         -9.6895e-01, -9.3385e-01,  8.7688e-02, -1.4384e-02, -1.6177e-01,\n",
       "         -6.5408e-01,  4.0277e-01, -3.3478e-01,  1.1858e-01, -6.6886e-01,\n",
       "         -7.0305e-01, -5.1441e-01, -9.5237e-01, -9.1525e-01,  1.1050e+00,\n",
       "          4.3446e-01, -1.0623e-02, -3.3998e-01, -2.3404e-02,  4.0167e-01,\n",
       "          3.9533e-01, -4.8314e-01,  4.3794e-01, -2.6641e-01,  3.8200e-01,\n",
       "         -4.8822e-01, -7.7889e-01,  1.6604e-01, -6.7790e-01,  3.8969e-01,\n",
       "          1.0133e+00,  2.6819e-01, -8.1425e-01,  1.8232e-01, -3.5920e-01,\n",
       "          5.6873e-01, -5.4350e-02, -4.8248e-01, -4.6289e-01,  1.7868e-03,\n",
       "         -1.8082e-01,  1.1802e+00, -5.1685e-01,  3.7303e-01,  4.7019e-01,\n",
       "          1.1272e+00,  4.4435e-01, -2.2998e-01, -2.0691e-02,  6.4403e-01,\n",
       "          5.6503e-01, -9.0920e-02,  1.3342e-01, -1.9040e-03,  3.8565e-01,\n",
       "         -3.0485e-01,  3.3966e-01,  9.1275e-02, -3.7259e-02,  1.9089e-01,\n",
       "         -3.9696e-02,  3.3477e-01,  1.0110e-02,  1.2703e-01,  3.1743e-01,\n",
       "         -9.4439e-01,  8.1628e-01,  1.8523e-01, -2.8612e-01,  6.0353e-01,\n",
       "          5.7859e-02, -3.9889e-01, -6.8153e-01, -2.1528e-03,  2.1654e-02,\n",
       "          8.7179e-01, -3.3681e-02,  2.9685e-01,  4.6491e-01, -3.7934e-01,\n",
       "          1.0363e-01, -2.4624e-01,  1.4175e-01, -6.0753e-01,  1.3404e-01,\n",
       "         -4.7139e-01, -6.0381e-01, -6.7441e-02,  1.8089e-01,  5.7821e-01,\n",
       "         -4.6450e-01, -1.7277e-01,  3.3293e-01, -1.1230e+00,  4.2833e-01,\n",
       "          5.4178e-01,  3.3899e-01,  1.2629e-01, -3.7323e-02,  7.4048e-01,\n",
       "         -3.4901e-01, -7.5029e-01, -2.5212e-01,  5.5385e-02, -1.4731e-01,\n",
       "          3.9292e-01,  9.0622e-01,  2.5193e-01, -7.2284e-01,  8.3003e-01,\n",
       "          1.6532e-03,  1.2776e-01,  2.1090e-01,  4.7954e-01,  2.8980e-01,\n",
       "          4.8430e-01, -5.3961e-01,  7.3651e-01,  5.0311e-01,  3.6418e-01,\n",
       "          9.5123e-02, -2.3112e-01, -6.9099e-02, -1.4634e-01,  7.9606e-02,\n",
       "         -7.7762e-01,  1.0900e+00,  1.1219e-01,  2.3925e-01, -3.2123e-01,\n",
       "          2.7129e-01,  3.1460e-01,  1.2289e+00, -1.3181e-01, -3.6957e-01,\n",
       "          6.5488e-01, -3.2460e-01,  3.3238e-01, -2.9674e-01, -1.3702e-01,\n",
       "         -7.8920e-01, -5.0539e-01, -3.1731e-01, -5.9946e-01,  4.3638e-01,\n",
       "          5.4328e-01, -2.2684e-01,  5.2308e-01, -2.2339e-01,  2.8445e-01,\n",
       "          5.2185e-01,  4.9945e-01,  1.2003e-01, -6.2690e-01,  8.7463e-01,\n",
       "         -1.1826e-01, -3.0798e-01,  4.6472e-01,  2.1289e-01, -4.3789e-01,\n",
       "         -2.9709e-01, -1.9381e-01,  8.5201e-02,  6.7717e-01, -2.2585e-01,\n",
       "          4.6445e-01,  1.0838e-01,  4.9734e-02, -2.7894e-01, -6.9735e-02,\n",
       "         -1.4102e+00, -1.8897e-01, -1.0531e+00,  3.9025e-01,  2.9710e-02,\n",
       "          2.5616e-01,  2.0413e-01, -5.8529e-01, -1.2881e-01,  2.8939e-01]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm = torch.fx.symbolic_trace(net)\n",
    "sample_input = torch.randn(1, 3 ,224, 224)\n",
    "ShapeProp(gm).propagate(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name                   target                                                   args                                   kwargs\n",
      "-------------  ---------------------  -------------------------------------------------------  -------------------------------------  --------\n",
      "placeholder    x                      x                                                        ()                                     {}\n",
      "call_module    conv1                  conv1                                                    (x,)                                   {}\n",
      "call_module    bn1                    bn1                                                      (conv1,)                               {}\n",
      "call_module    relu                   relu                                                     (bn1,)                                 {}\n",
      "call_module    maxpool                maxpool                                                  (relu,)                                {}\n",
      "call_module    layer1_0_conv1         layer1.0.conv1                                           (maxpool,)                             {}\n",
      "call_module    layer1_0_bn1           layer1.0.bn1                                             (layer1_0_conv1,)                      {}\n",
      "call_module    layer1_0_relu          layer1.0.relu                                            (layer1_0_bn1,)                        {}\n",
      "call_module    layer1_0_conv2         layer1.0.conv2                                           (layer1_0_relu,)                       {}\n",
      "call_module    layer1_0_bn2           layer1.0.bn2                                             (layer1_0_conv2,)                      {}\n",
      "call_function  add                    <built-in function add>                                  (layer1_0_bn2, maxpool)                {}\n",
      "call_module    layer1_0_relu_1        layer1.0.relu                                            (add,)                                 {}\n",
      "call_module    layer1_1_conv1         layer1.1.conv1                                           (layer1_0_relu_1,)                     {}\n",
      "call_module    layer1_1_bn1           layer1.1.bn1                                             (layer1_1_conv1,)                      {}\n",
      "call_module    layer1_1_relu          layer1.1.relu                                            (layer1_1_bn1,)                        {}\n",
      "call_module    layer1_1_conv2         layer1.1.conv2                                           (layer1_1_relu,)                       {}\n",
      "call_module    layer1_1_bn2           layer1.1.bn2                                             (layer1_1_conv2,)                      {}\n",
      "call_function  add_1                  <built-in function add>                                  (layer1_1_bn2, layer1_0_relu_1)        {}\n",
      "call_module    layer1_1_relu_1        layer1.1.relu                                            (add_1,)                               {}\n",
      "call_module    layer2_0_conv1         layer2.0.conv1                                           (layer1_1_relu_1,)                     {}\n",
      "call_module    layer2_0_bn1           layer2.0.bn1                                             (layer2_0_conv1,)                      {}\n",
      "call_module    layer2_0_relu          layer2.0.relu                                            (layer2_0_bn1,)                        {}\n",
      "call_module    layer2_0_conv2         layer2.0.conv2                                           (layer2_0_relu,)                       {}\n",
      "call_module    layer2_0_bn2           layer2.0.bn2                                             (layer2_0_conv2,)                      {}\n",
      "call_module    layer2_0_downsample_0  layer2.0.downsample.0                                    (layer1_1_relu_1,)                     {}\n",
      "call_module    layer2_0_downsample_1  layer2.0.downsample.1                                    (layer2_0_downsample_0,)               {}\n",
      "call_function  add_2                  <built-in function add>                                  (layer2_0_bn2, layer2_0_downsample_1)  {}\n",
      "call_module    layer2_0_relu_1        layer2.0.relu                                            (add_2,)                               {}\n",
      "call_module    layer2_1_conv1         layer2.1.conv1                                           (layer2_0_relu_1,)                     {}\n",
      "call_module    layer2_1_bn1           layer2.1.bn1                                             (layer2_1_conv1,)                      {}\n",
      "call_module    layer2_1_relu          layer2.1.relu                                            (layer2_1_bn1,)                        {}\n",
      "call_module    layer2_1_conv2         layer2.1.conv2                                           (layer2_1_relu,)                       {}\n",
      "call_module    layer2_1_bn2           layer2.1.bn2                                             (layer2_1_conv2,)                      {}\n",
      "call_function  add_3                  <built-in function add>                                  (layer2_1_bn2, layer2_0_relu_1)        {}\n",
      "call_module    layer2_1_relu_1        layer2.1.relu                                            (add_3,)                               {}\n",
      "call_module    layer3_0_conv1         layer3.0.conv1                                           (layer2_1_relu_1,)                     {}\n",
      "call_module    layer3_0_bn1           layer3.0.bn1                                             (layer3_0_conv1,)                      {}\n",
      "call_module    layer3_0_relu          layer3.0.relu                                            (layer3_0_bn1,)                        {}\n",
      "call_module    layer3_0_conv2         layer3.0.conv2                                           (layer3_0_relu,)                       {}\n",
      "call_module    layer3_0_bn2           layer3.0.bn2                                             (layer3_0_conv2,)                      {}\n",
      "call_module    layer3_0_downsample_0  layer3.0.downsample.0                                    (layer2_1_relu_1,)                     {}\n",
      "call_module    layer3_0_downsample_1  layer3.0.downsample.1                                    (layer3_0_downsample_0,)               {}\n",
      "call_function  add_4                  <built-in function add>                                  (layer3_0_bn2, layer3_0_downsample_1)  {}\n",
      "call_module    layer3_0_relu_1        layer3.0.relu                                            (add_4,)                               {}\n",
      "call_module    layer3_1_conv1         layer3.1.conv1                                           (layer3_0_relu_1,)                     {}\n",
      "call_module    layer3_1_bn1           layer3.1.bn1                                             (layer3_1_conv1,)                      {}\n",
      "call_module    layer3_1_relu          layer3.1.relu                                            (layer3_1_bn1,)                        {}\n",
      "call_module    layer3_1_conv2         layer3.1.conv2                                           (layer3_1_relu,)                       {}\n",
      "call_module    layer3_1_bn2           layer3.1.bn2                                             (layer3_1_conv2,)                      {}\n",
      "call_function  add_5                  <built-in function add>                                  (layer3_1_bn2, layer3_0_relu_1)        {}\n",
      "call_module    layer3_1_relu_1        layer3.1.relu                                            (add_5,)                               {}\n",
      "call_module    layer4_0_conv1         layer4.0.conv1                                           (layer3_1_relu_1,)                     {}\n",
      "call_module    layer4_0_bn1           layer4.0.bn1                                             (layer4_0_conv1,)                      {}\n",
      "call_module    layer4_0_relu          layer4.0.relu                                            (layer4_0_bn1,)                        {}\n",
      "call_module    layer4_0_conv2         layer4.0.conv2                                           (layer4_0_relu,)                       {}\n",
      "call_module    layer4_0_bn2           layer4.0.bn2                                             (layer4_0_conv2,)                      {}\n",
      "call_module    layer4_0_downsample_0  layer4.0.downsample.0                                    (layer3_1_relu_1,)                     {}\n",
      "call_module    layer4_0_downsample_1  layer4.0.downsample.1                                    (layer4_0_downsample_0,)               {}\n",
      "call_function  add_6                  <built-in function add>                                  (layer4_0_bn2, layer4_0_downsample_1)  {}\n",
      "call_module    layer4_0_relu_1        layer4.0.relu                                            (add_6,)                               {}\n",
      "call_module    layer4_1_conv1         layer4.1.conv1                                           (layer4_0_relu_1,)                     {}\n",
      "call_module    layer4_1_bn1           layer4.1.bn1                                             (layer4_1_conv1,)                      {}\n",
      "call_module    layer4_1_relu          layer4.1.relu                                            (layer4_1_bn1,)                        {}\n",
      "call_module    layer4_1_conv2         layer4.1.conv2                                           (layer4_1_relu,)                       {}\n",
      "call_module    layer4_1_bn2           layer4.1.bn2                                             (layer4_1_conv2,)                      {}\n",
      "call_function  add_7                  <built-in function add>                                  (layer4_1_bn2, layer4_0_relu_1)        {}\n",
      "call_module    layer4_1_relu_1        layer4.1.relu                                            (add_7,)                               {}\n",
      "call_module    avgpool                avgpool                                                  (layer4_1_relu_1,)                     {}\n",
      "call_function  flatten                <built-in method flatten of type object at 0x110dc11a0>  (avgpool, 1)                           {}\n",
      "call_module    fc                     fc                                                       (flatten,)                             {}\n",
      "output         output                 output                                                   (fc,)                                  {}\n"
     ]
    }
   ],
   "source": [
    "gm.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name                   target                                                   args                                   kwargs\n",
      "-------------  ---------------------  -------------------------------------------------------  -------------------------------------  --------\n",
      "placeholder    x                      x                                                        ()                                     {}\n",
      "call_module    conv1                  conv1                                                    (x,)                                   {}\n",
      "call_module    bn1                    bn1                                                      (conv1,)                               {}\n",
      "call_module    relu                   relu                                                     (bn1,)                                 {}\n",
      "call_module    maxpool                maxpool                                                  (relu,)                                {}\n",
      "call_module    layer1_0_conv1         layer1.0.conv1                                           (maxpool,)                             {}\n",
      "call_module    layer1_0_bn1           layer1.0.bn1                                             (layer1_0_conv1,)                      {}\n",
      "call_module    layer1_0_relu          layer1.0.relu                                            (layer1_0_bn1,)                        {}\n",
      "call_module    layer1_0_conv2         layer1.0.conv2                                           (layer1_0_relu,)                       {}\n",
      "call_module    layer1_0_bn2           layer1.0.bn2                                             (layer1_0_conv2,)                      {}\n",
      "call_function  add                    <built-in function add>                                  (layer1_0_bn2, maxpool)                {}\n",
      "call_module    layer1_0_relu_1        layer1.0.relu                                            (add,)                                 {}\n",
      "call_module    layer1_1_conv1         layer1.1.conv1                                           (layer1_0_relu_1,)                     {}\n",
      "call_module    layer1_1_bn1           layer1.1.bn1                                             (layer1_1_conv1,)                      {}\n",
      "call_module    layer1_1_relu          layer1.1.relu                                            (layer1_1_bn1,)                        {}\n",
      "call_module    layer1_1_conv2         layer1.1.conv2                                           (layer1_1_relu,)                       {}\n",
      "call_module    layer1_1_bn2           layer1.1.bn2                                             (layer1_1_conv2,)                      {}\n",
      "call_function  add_1                  <built-in function add>                                  (layer1_1_bn2, layer1_0_relu_1)        {}\n",
      "call_module    layer1_1_relu_1        layer1.1.relu                                            (add_1,)                               {}\n",
      "call_module    layer2_0_conv1         layer2.0.conv1                                           (layer1_1_relu_1,)                     {}\n",
      "call_module    layer2_0_bn1           layer2.0.bn1                                             (layer2_0_conv1,)                      {}\n",
      "call_module    layer2_0_relu          layer2.0.relu                                            (layer2_0_bn1,)                        {}\n",
      "call_module    layer2_0_conv2         layer2.0.conv2                                           (layer2_0_relu,)                       {}\n",
      "call_module    layer2_0_bn2           layer2.0.bn2                                             (layer2_0_conv2,)                      {}\n",
      "call_module    layer2_0_downsample_0  layer2.0.downsample.0                                    (layer1_1_relu_1,)                     {}\n",
      "call_module    layer2_0_downsample_1  layer2.0.downsample.1                                    (layer2_0_downsample_0,)               {}\n",
      "call_function  add_2                  <built-in function add>                                  (layer2_0_bn2, layer2_0_downsample_1)  {}\n",
      "call_module    layer2_0_relu_1        layer2.0.relu                                            (add_2,)                               {}\n",
      "call_module    layer2_1_conv1         layer2.1.conv1                                           (layer2_0_relu_1,)                     {}\n",
      "call_module    layer2_1_bn1           layer2.1.bn1                                             (layer2_1_conv1,)                      {}\n",
      "call_module    layer2_1_relu          layer2.1.relu                                            (layer2_1_bn1,)                        {}\n",
      "call_module    layer2_1_conv2         layer2.1.conv2                                           (layer2_1_relu,)                       {}\n",
      "call_module    layer2_1_bn2           layer2.1.bn2                                             (layer2_1_conv2,)                      {}\n",
      "call_function  add_3                  <built-in function add>                                  (layer2_1_bn2, layer2_0_relu_1)        {}\n",
      "call_module    layer2_1_relu_1        layer2.1.relu                                            (add_3,)                               {}\n",
      "call_module    layer3_0_conv1         layer3.0.conv1                                           (layer2_1_relu_1,)                     {}\n",
      "call_module    layer3_0_bn1           layer3.0.bn1                                             (layer3_0_conv1,)                      {}\n",
      "call_module    layer3_0_relu          layer3.0.relu                                            (layer3_0_bn1,)                        {}\n",
      "call_module    layer3_0_conv2         layer3.0.conv2                                           (layer3_0_relu,)                       {}\n",
      "call_module    layer3_0_bn2           layer3.0.bn2                                             (layer3_0_conv2,)                      {}\n",
      "call_module    layer3_0_downsample_0  layer3.0.downsample.0                                    (layer2_1_relu_1,)                     {}\n",
      "call_module    layer3_0_downsample_1  layer3.0.downsample.1                                    (layer3_0_downsample_0,)               {}\n",
      "call_function  add_4                  <built-in function add>                                  (layer3_0_bn2, layer3_0_downsample_1)  {}\n",
      "call_module    layer3_0_relu_1        layer3.0.relu                                            (add_4,)                               {}\n",
      "call_module    layer3_1_conv1         layer3.1.conv1                                           (layer3_0_relu_1,)                     {}\n",
      "call_module    layer3_1_bn1           layer3.1.bn1                                             (layer3_1_conv1,)                      {}\n",
      "call_module    layer3_1_relu          layer3.1.relu                                            (layer3_1_bn1,)                        {}\n",
      "call_module    layer3_1_conv2         layer3.1.conv2                                           (layer3_1_relu,)                       {}\n",
      "call_module    layer3_1_bn2           layer3.1.bn2                                             (layer3_1_conv2,)                      {}\n",
      "call_function  add_5                  <built-in function add>                                  (layer3_1_bn2, layer3_0_relu_1)        {}\n",
      "call_module    layer3_1_relu_1        layer3.1.relu                                            (add_5,)                               {}\n",
      "call_module    layer4_0_conv1         layer4.0.conv1                                           (layer3_1_relu_1,)                     {}\n",
      "call_module    layer4_0_bn1           layer4.0.bn1                                             (layer4_0_conv1,)                      {}\n",
      "call_module    layer4_0_relu          layer4.0.relu                                            (layer4_0_bn1,)                        {}\n",
      "call_module    layer4_0_conv2         layer4.0.conv2                                           (layer4_0_relu,)                       {}\n",
      "call_module    layer4_0_bn2           layer4.0.bn2                                             (layer4_0_conv2,)                      {}\n",
      "call_module    layer4_0_downsample_0  layer4.0.downsample.0                                    (layer3_1_relu_1,)                     {}\n",
      "call_module    layer4_0_downsample_1  layer4.0.downsample.1                                    (layer4_0_downsample_0,)               {}\n",
      "call_function  add_6                  <built-in function add>                                  (layer4_0_bn2, layer4_0_downsample_1)  {}\n",
      "call_module    layer4_0_relu_1        layer4.0.relu                                            (add_6,)                               {}\n",
      "call_module    layer4_1_conv1         layer4.1.conv1                                           (layer4_0_relu_1,)                     {}\n",
      "call_module    layer4_1_bn1           layer4.1.bn1                                             (layer4_1_conv1,)                      {}\n",
      "call_module    layer4_1_relu          layer4.1.relu                                            (layer4_1_bn1,)                        {}\n",
      "call_module    layer4_1_conv2         layer4.1.conv2                                           (layer4_1_relu,)                       {}\n",
      "call_module    layer4_1_bn2           layer4.1.bn2                                             (layer4_1_conv2,)                      {}\n",
      "call_function  add_7                  <built-in function add>                                  (layer4_1_bn2, layer4_0_relu_1)        {}\n",
      "call_module    layer4_1_relu_1        layer4.1.relu                                            (add_7,)                               {}\n",
      "call_module    avgpool                avgpool                                                  (layer4_1_relu_1,)                     {}\n",
      "call_function  flatten                <built-in method flatten of type object at 0x110dc11a0>  (avgpool, 1)                           {}\n",
      "call_module    fc                     fc                                                       (flatten,)                             {}\n",
      "output         output                 output                                                   (fc,)                                  {}\n"
     ]
    }
   ],
   "source": [
    "gm.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1,\tcall_module,\t params:torch.Size([64, 3, 7, 7]) out:torch.Size([1, 64, 112, 112])\n",
      "bn1,\tcall_module,\t params:torch.Size([64]) out:torch.Size([1, 64, 112, 112])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'relu.weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-4580a8a0625c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# print(f\"{node.target},\\t{node.op},\\t{node.meta['tensor_meta'].dtype},\\t{node.meta['tensor_meta'].shape}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"call_module\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{node.target},\\t{node.op},\\t params:{net.state_dict()[node.target + '.weight'].shape} out:{node.meta['tensor_meta'].shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'relu.weight'"
     ]
    }
   ],
   "source": [
    "for node in gm.graph.nodes:\n",
    "    # print(f\"{node.target},\\t{node.op},\\t{node.meta['tensor_meta'].dtype},\\t{node.meta['tensor_meta'].shape}\")\n",
    "    if node.op == \"call_module\":\n",
    "        print(f\"{node.target},\\t{node.op},\\t params:{net.state_dict()[node.target + '.weight'].shape} out:{node.meta['tensor_meta'].shape}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 3, 3])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['layer1.1.conv2.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMetadata(shape=torch.Size([1, 64, 112, 112]), dtype=torch.float32, stride=(802816, 12544, 112, 1), memory_format=torch.contiguous_format, is_quantized=False, qscheme=None, q_scale=None, q_zero_point=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node.meta['tensor_meta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TensorMetadata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-2c2318c805aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTensorMetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'TensorMetadata' is not defined"
     ]
    }
   ],
   "source": [
    "class ShapeProp(torch.fx.Interpreter):\n",
    "    def run_node(self, n : Node) -> Any:\n",
    "        result = super().run_node(n)\n",
    "        \n",
    "        found_tensor = False\n",
    "        def extract_tensor_meta(obj):\n",
    "            if isinstance(obj, torch.Tensor):\n",
    "                nonlocal found_tensor\n",
    "                found_tensor = True\n",
    "                return extract_tensor_metadata(obj)\n",
    "            else:\n",
    "                return obj\n",
    "\n",
    "        meta = map_aggregate(result, extract_tensor_meta)\n",
    "        if found_tensor:\n",
    "            n.meta['tensor_meta'] = meta\n",
    "\n",
    "        n.meta['type'] = type(result)\n",
    "        return result\n",
    "\n",
    "    def propagate(self, *args):\n",
    "        \"\"\"\n",
    "        Run `module` via interpretation and return the result and\n",
    "        record the shape and type of each node.\n",
    "        Args:\n",
    "            *args (Tensor): the sample input.\n",
    "        Returns:\n",
    "            Any: The value returned from executing the Module\n",
    "        \"\"\"\n",
    "        return super().run(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
