{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from itertools import product\n",
    "from collections import namedtuple\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import os\n",
    "import platform\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the original pytorch implementation of the class\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv1_bn=nn.BatchNorm2d(6)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        self.conv2_bn=nn.BatchNorm2d(12)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "        \n",
    "    def forward(self, t):\n",
    "       t = self.conv1(t)\n",
    "       t = self.conv1_bn(t)\n",
    "       t = F.max_pool2d(t, kernel_size =2, stride=2)\n",
    "       t = F.relu(t)\n",
    "       \n",
    "       t = self.conv2(t)\n",
    "       t = self.conv2_bn(t)\n",
    "       t = F.max_pool2d(t, kernel_size =2, stride=2)\n",
    "       t = F.relu(t)\n",
    "\n",
    "       t = t.reshape(-1, 12*4*4)\n",
    "       t = self.fc1(t)\n",
    "       t = F.relu(t)\n",
    "\n",
    "       t = self.fc2(t)\n",
    "       t = F.relu(t)\n",
    "\n",
    "       t = self.out(t)\n",
    "\n",
    "       return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NetworkLightning(pl.LightningModule):\n",
    "    def __init__(self, model: nn.Module,learning_rate: float):\n",
    "        super(NetworkLightning, self).__init__()\n",
    "        self.network = Network()\n",
    "        # Dummy array to get computational graph\n",
    "        self.example_input_array =torch.empty(1,1,28,28)\n",
    "        self.save_hyperparameters()\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images = batch[0]\n",
    "        labels = batch[1]\n",
    "\n",
    "        preds = self.network(images)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "                # logs- a dictionary \n",
    "        self.log('trn_loss', loss.item())\n",
    " \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images = batch[0]\n",
    "        labels = batch[1]\n",
    "\n",
    "        preds = self.network(images)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        self.log('val_loss', loss.item(), prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "        \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets now also pull in our run builder class\n",
    "class RunBuilder():\n",
    "    @staticmethod\n",
    "    def get_runs(params):\n",
    "        # \n",
    "        Run = namedtuple('Run', params.keys())\n",
    "\n",
    "        runs = []\n",
    "        for vals in product(*params.values()):\n",
    "            runs.append(Run(*vals))\n",
    "        return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets re-instantiate our normalized train set\n",
    "\n",
    "mean = 0.2860\n",
    "std = 0.3530\n",
    "\n",
    "valid_ratio = 0.2  # Going to use 80%/20% split for train/valid\n",
    "\n",
    "train_valid_set = torchvision.datasets.FashionMNIST(\n",
    "    root='/home/slabban/machine_learning/machine_learning_courses/datasets'\n",
    "    ,train=True\n",
    "    ,download=True\n",
    "    ,transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)])\n",
    ")\n",
    "\n",
    "nb_train = int((1.0 - valid_ratio) * len(train_valid_set))\n",
    "nb_valid =  int(valid_ratio * len(train_valid_set))\n",
    "train_set, valid_set = torch.utils.data.dataset.random_split(train_valid_set, [nb_train, nb_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets now also pull in our run builder class\n",
    "class RunBuilder():\n",
    "    @staticmethod\n",
    "    def get_runs(params):\n",
    "        # \n",
    "        Run = namedtuple('Run', params.keys())\n",
    "\n",
    "        runs = []\n",
    "        for vals in product(*params.values()):\n",
    "            runs.append(Run(*vals))\n",
    "        return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "\n",
    "## log and print\n",
    "\n",
    "class Log_and_print():\n",
    "    # need this to ensure that stuff are printed to STDOUT as well for backup\n",
    "    '''\n",
    "    A simple logging mecahnism for arbitary timestamped messages during the fit routine\n",
    "\n",
    "    https://stackoverflow.com/questions/45016458/tensorflow-tf-summary-text-and-linebreaks\n",
    "    Tensorboard text uses the markdown format.\n",
    "    That means you need to add 2 spaces before \\n to produce a linebreak\n",
    "    '''\n",
    "    def __init__(self, tb_logger):\n",
    "        self.tb_logger = tb_logger\n",
    "        self.str_log = ('PARTIAL COPY OF TEXT LOG TO TENSORBOARD TEXT  \\n'\n",
    "                        'class Log_and_print() by Arian Prabowo  \\n'\n",
    "                        'RUN NAME: ')\n",
    "\n",
    "    def lnp(self, tag):\n",
    "        print(time.asctime(), tag)\n",
    "        self.str_log += str(time.asctime()) + ' ' + str(tag) + '  \\n'\n",
    "    \n",
    "\n",
    "\n",
    "## LogParameters\n",
    "\n",
    "class LogParameters(pl.Callback):\n",
    "    \"\"\" This is a pytorch lightning callback class that logs the weight and biases to tensorbard\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def on_fit_start(self, trainer, pl_module):\n",
    "        self.d_parameters = {}\n",
    "        for n,p in pl_module.named_parameters():\n",
    "            self.d_parameters[n] = []\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        if not trainer.sanity_checking: # WARN: sanity_check is turned on by default\n",
    "            lp = []\n",
    "            for n,p in pl_module.named_parameters():\n",
    "                trainer.logger.experiment.add_histogram(n, p.data, trainer.current_epoch)\n",
    "                self.d_parameters[n].append(p.ravel().cpu().numpy())\n",
    "                lp.append(p.ravel().cpu().numpy())\n",
    "            p = np.concatenate(lp)\n",
    "            trainer.logger.experiment.add_histogram('Parameters', p, trainer.current_epoch)\n",
    "\n",
    "## LogHyperParameters\n",
    "\n",
    "class LogHyperparameters(pl.Callback):\n",
    "    \"\"\" This is a pytorch lightning callback class that logs high-level run hyperparameters\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def on_fit_start(self, trainer, pl_module):\n",
    "        trainer.logger.log_hyperparams(pl_module.hparams)\n",
    "\n",
    "## LogComputationalGraph\n",
    "class LogComputationalGraph(pl.Callback):\n",
    "    \"\"\" This is a pytorch lightning callback class that plots the computational graph\n",
    "    Arguments:\n",
    "        dummy_input: the graph needs this dummy input to compute the graph\n",
    "\n",
    "    NOTE: This is currently throwing errors due to some deprecations\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "\n",
    "    def on_fit_start(self, trainer, pl_module):\n",
    "        trainer.logger.experiment.add_graph(pl_module, pl_module.example_input_array)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_main(parameters: OrderedDict):\n",
    "    \"\"\" This class \"\"\"\n",
    "\n",
    "\n",
    "    tb_logger = TensorBoardLogger(\"lightning_logs\", log_graph=True, )\n",
    "\n",
    "\n",
    "    lnp = Log_and_print(tb_logger=tb_logger)\n",
    "    lnp.lnp('Loggers start')\n",
    "\n",
    "    l_callbacks = []\n",
    "\n",
    "    bLogParameters = LogParameters()\n",
    "    l_callbacks.append(bLogParameters)\n",
    "    bLog_Hyperparameters = LogHyperparameters()\n",
    "    l_callbacks.append(bLog_Hyperparameters)\n",
    "\n",
    "    model = Network()\n",
    "\n",
    "    #bLogCompGraph = LogComputationalGraph()\n",
    "    #l_callbacks.append(bLogCompGraph)\n",
    "\n",
    "\n",
    "    \n",
    "    for run in RunBuilder.get_runs(parameters):\n",
    "        # TODO: Integrate this\n",
    "        # Instantiate the model\n",
    "        model = NetworkLightning(model= model,learning_rate=run.lr)\n",
    "\n",
    "\n",
    "\n",
    "        # Create a data loader\n",
    "        train_loader = DataLoader(train_set, batch_size=run.batch_size, shuffle=run.shuffle, num_workers=run.num_workers)\n",
    "        val_loader = DataLoader(valid_set, batch_size=run.batch_size, shuffle=False, num_workers=run.num_workers)\n",
    "\n",
    "\n",
    "\n",
    "        trainer = pl.Trainer(max_epochs=run.epochs, logger=tb_logger, accelerator=run.device, devices=1, callbacks=l_callbacks, log_every_n_steps= 3)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = OrderedDict(\n",
    "    lr = [.0001, 0.01]\n",
    "    ,batch_size = [1000]\n",
    "    , num_workers = [10]\n",
    "    , device = ['gpu']\n",
    "    , shuffle = [True]\n",
    "    , epochs = [3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slabban/anaconda3/envs/pytorch-fastai/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:262: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type    | Params | In sizes       | Out sizes\n",
      "-----------------------------------------------------------------\n",
      "0 | network | Network | 33.0 K | [1, 1, 28, 28] | [1, 10]  \n",
      "-----------------------------------------------------------------\n",
      "33.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "33.0 K    Total params\n",
      "0.132     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan 22 14:58:40 2023 Loggers start\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52062fee28643f187f78b85bf1c2efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e325fc22ea34e01b4e8757b098ea94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b775bace3c4c2986ce5bb143fd4bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e14286355294930b3513076fbaa3f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5181eefe26146ffbd14f3e4b5f627d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/slabban/anaconda3/envs/pytorch-fastai/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:604: UserWarning: Checkpoint directory lightning_logs/lightning_logs/version_10/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type    | Params | In sizes       | Out sizes\n",
      "-----------------------------------------------------------------\n",
      "0 | network | Network | 33.0 K | [1, 1, 28, 28] | [1, 10]  \n",
      "-----------------------------------------------------------------\n",
      "33.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "33.0 K    Total params\n",
      "0.132     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2341ab40f7642cb88d13dee01ea8fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4292cbe162846df9361d8de4163f783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a1bfe673dd4194a4bda99bdc0d715a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slabban/anaconda3/envs/pytorch-fastai/lib/python3.9/site-packages/lightning_lite/plugins/io/torch_io.py:61: UserWarning: Warning, `hyper_parameters` dropped from checkpoint. An attribute is not picklable: Can't pickle local object 'EvaluationLoop.advance.<locals>.batch_to_device'\n",
      "  rank_zero_warn(f\"Warning, `{key}` dropped from checkpoint. An attribute is not picklable: {err}\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0a0f71c7644b0f85bd06f9f7ebd967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b67b4709894473bc49ba7a8173ad84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer_main(parameters=parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87c69a23efa0ee1d5c64982a8e9486c08d519e3aa99d691b2ceccc04345ec1d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
